<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="qSJUmJp4-T1Lz9aVN5PqYy0RLyyLWAThSD6RTUty4S8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A PINN for your Price | Simon Ellersgaard Nielsen</title> <meta name="author" content="Simon Ellersgaard Nielsen"> <meta name="description" content="Solving the Black-Scholes PDE with physics inspired neural networks"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sellersgaard.github.io/blog/2024/pinn/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Simon </span>Ellersgaard Nielsen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">A PINN for your Price</h1> <p class="post-meta">January 16, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/options"> <i class="fas fa-hashtag fa-sm"></i> options</a>   <a href="/blog/tag/stochastic-calculus"> <i class="fas fa-hashtag fa-sm"></i> stochastic-calculus</a>   <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> machine-learning</a>     ·   <a href="/blog/category/pricing"> <i class="fas fa-tag fa-sm"></i> pricing</a>   </p> </header> <article class="post-content"> <figure> <picture> <img src="/assets/img/pinn1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="deep-learning-with-physics">Deep Learning with Physics</h3> <p>An oft-repeated exercise is to fit an artificial neural network (ANN) to an array of ex ante given option prices, thereby establishing that multi-parameter non-linear functions can learn the <a href="https://en.wikipedia.%0Aorg/wiki/Black%E2%80%93Scholes_model" rel="external nofollow noopener" target="_blank">Black-Scholes formula</a>. While numerically pleasing, this fact follows trivially from the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="external nofollow noopener" target="_blank">Universal Approximation Theorem</a>. Furthermore, it is putting the <em>cart before the horse</em> as it were: ideally we would want our network to price options <em>without</em> giving it access to the very price data we are after. Can it be done?</p> <p>Thanks to <a href="https://maziarraissi.github.io/PINNs/" rel="external nofollow noopener" target="_blank">Physics Inspired Neural Networks</a> by Raissi, Perdikaris, and Karniadakis the answer appears to be in the affirmative. Their general idea is as follows: doing theoretical physics effectively boils down to (i) postulating some model, and (ii) solving the governing laws of motion thereof, expressed as partial differential equations with given boundary conditions. Something along the lines of:</p> <p>\begin{equation}\label{pde} \frac{\partial u(t, \boldsymbol{x})}{\partial t} + \mathcal{D}[u(t, \boldsymbol{x});\boldsymbol{\lambda}] = 0, \end{equation}</p> <p>where \(u: [0,T] \times \Omega \mapsto \mathbb{R}^d\) is the latent (hidden) solution, and \(\mathcal{D}[u(t, \boldsymbol {x});\boldsymbol{\lambda}]\) is a (possibly non-linear) differential operator parameterised by \(\boldsymbol{\lambda}\), with \(u\) being subject to boundary conditions à la \(u(0,\boldsymbol{x}) = g(\boldsymbol{x})\) et cetera.</p> <p>Suppose we desire an ANN approximation \(\mathfrak{u}(t, \boldsymbol{x})\) of \(u(t, \boldsymbol{x})\). Traditionally that would entail solving \eqref {pde} first and then feeding the resulting data to the network during its training process. However, as observed by Raissi et al., it may suffice to expose the network to the PDE directly. Specifically, part of the learning objective could be to minimise the mean squared error of \(\mathfrak{f} \equiv \partial_t \mathfrak{u} + \mathcal{D}[\mathfrak{u}; \boldsymbol {\lambda}]\) where the partial derivatives are evaluated using <a href="https://en.wikipedia.%0Aorg/wiki/Automatic_differentiation" rel="external nofollow noopener" target="_blank">automatic differentiation</a>, for some set of randomly generated coordinates \(\mathbb{F} \equiv \{ (t_i^f, \boldsymbol{x}_i^f) \} \vert_{i=1}^{N_f}\) \(\subset [0,T] \times \Omega\). Meanwhile, we obviously want to respect the boundary conditions: to this end we generate a second set of coordinates \(\mathbb{U} \equiv \{ (t_i^u, \boldsymbol{x}_i^u, u_i^u) \} \vert_{i=1}^{N_u}\) on the boundary, adding to the cost function the mean squared error of the \(\mathfrak{u}\)s (recall, the boundary \(u\)s are known a priori). Altogether, our learning objective is therefore to find the set of neural network parameters \(\{ (\boldsymbol{W}^{[l]}, \boldsymbol{\beta}^{[l]}) \}_{l=0}^K\) which minimises</p> \[\text{cost} \equiv \frac{1}{N_u} \sum_{i=1}^{N_u} (\mathfrak{u}(t_i^u, \boldsymbol{x}_i^u) - u_i^u)^2 + \frac{1}{N_f} \sum_{i=1}^{N_f} (\mathfrak{f}(t_i^u, \boldsymbol{x}_i^u))^2.\] <p>Raissi et al. go on to demonstrate that they obtain very decent performance for problems pertaining to <a href="https://en.wikipedia.org/wiki/Burgers%27_equation" rel="external nofollow noopener" target="_blank">Burgers’ equation</a> and the <a href="https://en.wikipedia.%0Aorg/wiki/Schr%C3%B6dinger_equation" rel="external nofollow noopener" target="_blank">Schrödinger equation</a>.</p> <p>Quants, the most <a href="https://en.wikipedia.org/wiki/Physics_envy" rel="external nofollow noopener" target="_blank">physics envious</a> of all, will surely take heed of this result. In the section below I consider the pricing of European call options using PINNs, and reflect upon the broader viability of the methodology.</p> <h3 id="options-pricing-with-pinns">Options Pricing with PINNs</h3> <p>Recall that in a Black-Scholes world where the underlying security follows <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion" rel="external nofollow noopener" target="_blank">geometric Brownian motion</a> the no-arbitrage price \(u(t,S_t)\) of a European option obeys the PDE</p> \[\frac{\partial u(t,S)}{\partial t} + r S \frac{\partial u(t,S)}{\partial S} + \tfrac{1}{2} \sigma^2 S^2 \frac{\partial^2 u(t,S)}{\partial S^2} - ru(t,S) = 0,\] <p>where \(r\) is the risk-free rate, and \(\sigma\) is the volatility. For a plain vanilla call option with maturity \(T\) and strike \(K\), the appropriate boundary conditions are \(u(T,S_T) = \max \{S_T - K, 0 \}\), \(u(t,0) = 0\), and \(\lim_{S \rightarrow \infty} u(t,S_t) = S_t - e^{-r(T-t)}K\).</p> <p>To model this I consider a neural network with <a href="https://pytorch.org/docs/stable/generated/torch.nn.%0AReLU.html" rel="external nofollow noopener" target="_blank">ReLU activation</a> functions and three <a href="https://deepai.%0Aorg/machine-learning-glossary-and-terms/hidden-layer-machine-learning" rel="external nofollow noopener" target="_blank">hidden layers</a>, each of size 100 (amounting to 20,601 trainable parameters). The network is implemented in PyTorch with its convenient <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" rel="external nofollow noopener" target="_blank">autograd functionality</a>, and the cost function is optimised using <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="external nofollow noopener" target="_blank">L-BFGS</a>; a quasi-Newton fullbatch gradient-based optimization algorithm. Other parameters are specified below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.04</span><span class="p">,</span> <span class="sh">'</span><span class="s">σ</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sh">'</span><span class="s">τ</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
    <span class="sh">'</span><span class="s">layers</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span> <span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1">#
</span><span class="p">}</span>

<span class="n">tsteps</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># Number of steps in the time direction
</span><span class="n">xsteps</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># Number of steps in the space direction
</span><span class="n">N_u</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># Number of observable (boundary) points for training. I.e. complete (t,S,C) tuples
</span><span class="n">N_f</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c1"># Number of randomly generated points
</span><span class="n">S_min</span> <span class="o">=</span> <span class="mf">1e-20</span> <span class="c1"># min and max space direction
</span><span class="n">S_max</span> <span class="o">=</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">τ</span> <span class="o">+</span> <span class="n">σ</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">τ_min</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># min time
</span></code></pre></div></div> <p>Upon training the network for some minutes on my humble laptop, the calibration starts to converge:</p> <figure> <picture> <img src="/assets/img/pinn2.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The network understands that deep out-of-the-money options are effectively worthless, while deep-in-the-money options scale as \(\sim S-K\). There is also an appreciation that price levels increase with the time to maturity as shown in the \((t,S_t)\) heatmap below (the red lines are contours; the black line the strike price):</p> <figure> <picture> <img src="/assets/img/pinn3.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>However, that does not mean that the calibration is flawless. For some of the temporal snapshots below, there’s a distinct lack of <a href="https://en.wikipedia.org/wiki/Convex_function" rel="external nofollow noopener" target="_blank">convexity</a> in the predicted price function.</p> <figure> <picture> <img src="/assets/img/pinn4.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In fact, there are quite a number of issues with this type of pricing process, including, but not limited to</p> <ul> <li>The comparative slowness of the procedure vis-à-vis more established numerical methods (Monte Carlo, finite differences).</li> <li>The difficulty associated with precluding arbitrage opportunities.</li> <li>The sensitivity of the output with respect to neural network architecture, including the choice of activation function.</li> <li>Getting stuck in local minima of the cost function.</li> </ul> <p>Nonetheless, I am reasonably excited about the possibilities PINNs bring to quantitative finance: especially in the context of solving <em>non-linear</em> PDEs pertaining to pricing or optimal control (e.g. <a href="https://en.wikipedia.org/wiki/Merton%27s_portfolio_problem" rel="external nofollow noopener" target="_blank">optimal portfolio problems</a>).</p> <h3 id="code-reference">Code Reference</h3> <p>The calibration was done using the code snippets below. The code is a modified version based on work by Raissi et al.. If you find yourself playing around with this stuff and manage to do something interesting/get better fits, I’d like to hear about it!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DNN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A simple class for setting up and stepping through a neural network 
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
        
        <span class="nf">super</span><span class="p">(</span><span class="n">DNN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># parameters
</span>        <span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        
        <span class="c1"># set up layer order dict
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span>
        
        <span class="n">layer_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span> 
            <span class="n">layer_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="p">(</span><span class="sh">'</span><span class="s">layer_%d</span><span class="sh">'</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
            <span class="n">layer_list</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="sh">'</span><span class="s">activation_%d</span><span class="sh">'</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">()))</span>
            
        <span class="n">layer_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
            <span class="p">(</span><span class="sh">'</span><span class="s">layer_%d</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="p">)</span>
        
        <span class="n">layerDict</span> <span class="o">=</span> <span class="nc">OrderedDict</span><span class="p">(</span><span class="n">layer_list</span><span class="p">)</span>
        
        <span class="c1"># deploy layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">layerDict</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PhysicsInformedNN</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Physics inspired neural network for Black Scholes
    For other PDEs adjust self.net_f and input values
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_u</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> 
                         <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span>
                         <span class="n">X_f</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> 
                         <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> 
                         <span class="n">lb</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span>
                         <span class="n">ub</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        
        <span class="c1"># boundaries
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">lb</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ub</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">ub</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
               
        <span class="c1"># data
</span>        <span class="n">self</span><span class="p">.</span><span class="n">x_u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_u</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">t_u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_u</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_f</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">t_f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_f</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">u</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># network configurations
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">layers</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]</span>
        
        <span class="c1"># deep neural networks
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dnn</span> <span class="o">=</span> <span class="nc">DNN</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># optimizers: using the same settings
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">LBFGS</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> 
            <span class="n">lr</span><span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="c1">#1.0, 
</span>            <span class="n">max_iter</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> 
            <span class="n">max_eval</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> 
            <span class="n">history_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">tolerance_grad</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> 
            <span class="n">tolerance_change</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">).</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">line_search_fn</span><span class="o">=</span><span class="sh">"</span><span class="s">strong_wolfe</span><span class="sh">"</span>       <span class="c1"># can be "strong_wolfe"
</span>        <span class="p">)</span> <span class="c1"># https://en.wikipedia.org/wiki/Limited-memory_BFGS
</span>        
        <span class="c1">#
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ierr</span> <span class="o">=</span> <span class="p">{}</span>
           
    <span class="k">def</span> <span class="nf">_net_u</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>  
        <span class="sh">"""</span><span class="s"> boudary: nn value </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dnn</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">_net_f</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> interior: nn PDE </span><span class="sh">"""</span>
        
        <span class="n">u</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">u_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> 
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">u</span><span class="p">),</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">u_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> 
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">u</span><span class="p">),</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">u_xx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span>
            <span class="n">u_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> 
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">u_x</span><span class="p">),</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">u_t</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">u_x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="nf">pow</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">σ</span><span class="sh">'</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="n">u_xx</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">u</span>     
  
    <span class="k">def</span> <span class="nf">_loss_func</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        
        <span class="n">u_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_u</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_u</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">t_u</span><span class="p">)</span>
        <span class="n">f_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_f</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_f</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">t_f</span><span class="p">)</span>
        <span class="n">loss_u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">u</span> <span class="o">-</span> <span class="n">u_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">loss_f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">f_pred</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_u</span> <span class="o">+</span> <span class="n">loss_f</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nb">iter</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span>
                <span class="sh">'</span><span class="s">Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">loss_u</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">loss_f</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ierr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="sh">'</span><span class="s">loss_u</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss_u</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="sh">'</span><span class="s">loss_f</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss_f</span><span class="p">.</span><span class="nf">item</span><span class="p">()}</span>    
        
        <span class="k">return</span> <span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>        
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_loss_func</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">f</span>
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Simon Ellersgaard Nielsen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>