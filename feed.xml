<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sellersgaard.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sellersgaard.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-16T20:56:12+00:00</updated><id>https://sellersgaard.github.io/feed.xml</id><title type="html">blank</title><subtitle>Quantitative musings. </subtitle><entry><title type="html">A PINN for your Price</title><link href="https://sellersgaard.github.io/blog/2024/pinn/" rel="alternate" type="text/html" title="A PINN for your Price"/><published>2024-01-16T11:00:00+00:00</published><updated>2024-01-16T11:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2024/pinn</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2024/pinn/"><![CDATA[<figure> <picture> <img src="/assets/img/pinn1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="deep-learning-with-physics">Deep Learning with Physics</h3> <p>An oft-repeated exercise is to fit an artificial neural network (ANN) to an array of ex ante given option prices, thereby establishing that multi-parameter non-linear functions can learn the <a href="https://en.wikipedia. org/wiki/Black%E2%80%93Scholes_model">Black-Scholes formula</a>. While numerically pleasing, this fact follows trivially from the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a>. Furthermore, it is putting the <em>cart before the horse</em> as it were: ideally we would want our network to price options <em>without</em> giving it access to the very price data we are after. Can it be done?</p> <p>Thanks to <a href="https://maziarraissi.github.io/PINNs/">Physics Inspired Neural Networks</a> by Raissi, Perdikaris, and Karniadakis the answer appears to be in the affirmative. Their general idea is as follows: doing theoretical physics effectively boils down to (i) postulating some model, and (ii) solving the governing laws of motion thereof, expressed as partial differential equations with given boundary conditions. Something along the lines of:</p> <p>\begin{equation}\label{pde} \frac{\partial u(t, \boldsymbol{x})}{\partial t} + \mathcal{D}[u(t, \boldsymbol{x});\boldsymbol{\lambda}] = 0, \end{equation}</p> <p>where \(u: [0,T] \times \Omega \mapsto \mathbb{R}^d\) is the latent (hidden) solution, and \(\mathcal{D}[u(t, \boldsymbol {x});\boldsymbol{\lambda}]\) is a (possibly non-linear) differential operator parameterised by \(\boldsymbol{\lambda}\), with \(u\) being subject to boundary conditions à la \(u(0,\boldsymbol{x}) = g(\boldsymbol{x})\) et cetera.</p> <p>Suppose we desire an ANN approximation \(\mathfrak{u}(t, \boldsymbol{x})\) of \(u(t, \boldsymbol{x})\). Traditionally that would entail solving \eqref {pde} first and then feeding the resulting data to the network during its training process. However, as observed by Raissi et al., it may suffice to expose the network to the PDE directly. Specifically, part of the learning objective could be to minimise the mean squared error of \(\mathfrak{f} \equiv \partial_t \mathfrak{u} + \mathcal{D}[\mathfrak{u}; \boldsymbol {\lambda}]\) where the partial derivatives are evaluated using <a href="https://en.wikipedia. org/wiki/Automatic_differentiation">automatic differentiation</a>, for some set of randomly generated coordinates \(\mathbb{F} \equiv \{ (t_i^f, \boldsymbol{x}_i^f) \} \vert_{i=1}^{N_f}\) \(\subset [0,T] \times \Omega\). Meanwhile, we obviously want to respect the boundary conditions: to this end we generate a second set of coordinates \(\mathbb{U} \equiv \{ (t_i^u, \boldsymbol{x}_i^u, u_i^u) \} \vert_{i=1}^{N_u}\) on the boundary, adding to the cost function the mean squared error of the \(\mathfrak{u}\)s (recall, the boundary \(u\)s are known a priori). Altogether, our learning objective is therefore to find the set of neural network parameters \(\{ (\boldsymbol{W}^{[l]}, \boldsymbol{\beta}^{[l]}) \}_{l=0}^K\) which minimises</p> \[\text{cost} \equiv \frac{1}{N_u} \sum_{i=1}^{N_u} (\mathfrak{u}(t_i^u, \boldsymbol{x}_i^u) - u_i^u)^2 + \frac{1}{N_f} \sum_{i=1}^{N_f} (\mathfrak{f}(t_i^u, \boldsymbol{x}_i^u))^2.\] <p>Raissi et al. go on to demonstrate that they obtain very decent performance for problems pertaining to <a href="https://en.wikipedia.org/wiki/Burgers%27_equation">Burgers’ equation</a> and the <a href="https://en.wikipedia. org/wiki/Schr%C3%B6dinger_equation">Schrödinger equation</a>.</p> <p>Quants, the most <a href="https://en.wikipedia.org/wiki/Physics_envy">physics envious</a> of all, will surely take heed of this result. In the section below I consider the pricing of European call options using PINNs, and reflect upon the broader viability of the methodology.</p> <h3 id="options-pricing-with-pinns">Options Pricing with PINNs</h3> <p>Recall that in a Black-Scholes world where the underlying security follows <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion">geometric Brownian motion</a> the no-arbitrage price \(u(t,S_t)\) of a European option obeys the PDE</p> \[\frac{\partial u(t,S)}{\partial t} + r S \frac{\partial u(t,S)}{\partial S} + \tfrac{1}{2} \sigma^2 S^2 \frac{\partial^2 u(t,S)}{\partial S^2} - ru(t,S) = 0,\] <p>where \(r\) is the risk-free rate, and \(\sigma\) is the volatility. For a plain vanilla call option with maturity \(T\) and strike \(K\), the appropriate boundary conditions are \(u(T,S_T) = \max \{S_T - K, 0 \}\), \(u(t,0) = 0\), and \(\lim_{S \rightarrow \infty} u(t,S_t) = S_t - e^{-r(T-t)}K\).</p> <p>To model this I consider a neural network with <a href="https://pytorch.org/docs/stable/generated/torch.nn. ReLU.html">ReLU activation</a> functions and three <a href="https://deepai. org/machine-learning-glossary-and-terms/hidden-layer-machine-learning">hidden layers</a>, each of size 100 (amounting to 20,601 trainable parameters). The network is implemented in PyTorch with its convenient <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">autograd functionality</a>, and the cost function is optimised using <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a>; a quasi-Newton fullbatch gradient-based optimization algorithm. Other parameters are specified below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.04</span><span class="p">,</span> <span class="sh">'</span><span class="s">σ</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sh">'</span><span class="s">τ</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
    <span class="sh">'</span><span class="s">layers</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span> <span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1">#
</span><span class="p">}</span>

<span class="n">tsteps</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># Number of steps in the time direction
</span><span class="n">xsteps</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># Number of steps in the space direction
</span><span class="n">N_u</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># Number of observable (boundary) points for training. I.e. complete (t,S,C) tuples
</span><span class="n">N_f</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c1"># Number of randomly generated points
</span><span class="n">S_min</span> <span class="o">=</span> <span class="mf">1e-20</span> <span class="c1"># min and max space direction
</span><span class="n">S_max</span> <span class="o">=</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">τ</span> <span class="o">+</span> <span class="n">σ</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">τ_min</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># min time
</span></code></pre></div></div> <p>Upon training the network for some minutes on my humble laptop, the calibration starts to converge:</p> <figure> <picture> <img src="/assets/img/pinn2.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The network understands that deep out-of-the-money options are effectively worthless, while deep-in-the-money options scale as \(\sim S-K\). There is also an appreciation that price levels increase with the time to maturity as shown in the \((t,S_t)\) heatmap below (the red lines are contours; the black line the strike price):</p> <figure> <picture> <img src="/assets/img/pinn3.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>However, that does not mean that the calibration is flawless. For some of the temporal snapshots below, there’s a distinct lack of <a href="https://en.wikipedia.org/wiki/Convex_function">convexity</a> in the predicted price function.</p> <figure> <picture> <img src="/assets/img/pinn4.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In fact, there are quite a number of issues with this type of pricing process, including, but not limited to</p> <ul> <li>The comparative slowness of the procedure vis-à-vis more established numerical methods (Monte Carlo, finite differences).</li> <li>The difficulty associated with precluding arbitrage opportunities.</li> <li>The sensitivity of the output with respect to neural network architecture, including the choice of activation function.</li> <li>Getting stuck in local minima of the cost function.</li> </ul> <p>Nonetheless, I am reasonably excited about the possibilities PINNs bring to quantitative finance: especially in the context of solving <em>non-linear</em> PDEs pertaining to pricing or optimal control (e.g. <a href="https://en.wikipedia.org/wiki/Merton%27s_portfolio_problem">optimal portfolio problems</a>).</p> <h3 id="code-reference">Code Reference</h3> <p>The calibration was done using the code snippets below. The code is a modified version based on work by Raissi et al.. If you find yourself playing around with this stuff and manage to do something interesting/get better fits, I’d like to hear about it!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DNN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A simple class for setting up and stepping through a neural network 
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
        
        <span class="nf">super</span><span class="p">(</span><span class="n">DNN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># parameters
</span>        <span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        
        <span class="c1"># set up layer order dict
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span>
        
        <span class="n">layer_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span> 
            <span class="n">layer_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="p">(</span><span class="sh">'</span><span class="s">layer_%d</span><span class="sh">'</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>
            <span class="n">layer_list</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="sh">'</span><span class="s">activation_%d</span><span class="sh">'</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">()))</span>
            
        <span class="n">layer_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
            <span class="p">(</span><span class="sh">'</span><span class="s">layer_%d</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="p">)</span>
        
        <span class="n">layerDict</span> <span class="o">=</span> <span class="nc">OrderedDict</span><span class="p">(</span><span class="n">layer_list</span><span class="p">)</span>
        
        <span class="c1"># deploy layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">layerDict</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PhysicsInformedNN</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Physics inspired neural network for Black Scholes
    For other PDEs adjust self.net_f and input values
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_u</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> 
                         <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span>
                         <span class="n">X_f</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> 
                         <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> 
                         <span class="n">lb</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span>
                         <span class="n">ub</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        
        <span class="c1"># boundaries
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">lb</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ub</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">ub</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
               
        <span class="c1"># data
</span>        <span class="n">self</span><span class="p">.</span><span class="n">x_u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_u</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">t_u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_u</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_f</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">t_f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_f</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">u</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># network configurations
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">layers</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]</span>
        
        <span class="c1"># deep neural networks
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dnn</span> <span class="o">=</span> <span class="nc">DNN</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># optimizers: using the same settings
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">LBFGS</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> 
            <span class="n">lr</span><span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="c1">#1.0, 
</span>            <span class="n">max_iter</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> 
            <span class="n">max_eval</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> 
            <span class="n">history_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">tolerance_grad</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> 
            <span class="n">tolerance_change</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">).</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">line_search_fn</span><span class="o">=</span><span class="sh">"</span><span class="s">strong_wolfe</span><span class="sh">"</span>       <span class="c1"># can be "strong_wolfe"
</span>        <span class="p">)</span> <span class="c1"># https://en.wikipedia.org/wiki/Limited-memory_BFGS
</span>        
        <span class="c1">#
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ierr</span> <span class="o">=</span> <span class="p">{}</span>
           
    <span class="k">def</span> <span class="nf">_net_u</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>  
        <span class="sh">"""</span><span class="s"> boudary: nn value </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dnn</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">_net_f</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> interior: nn PDE </span><span class="sh">"""</span>
        
        <span class="n">u</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">u_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> 
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">u</span><span class="p">),</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">u_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> 
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">u</span><span class="p">),</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">u_xx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span>
            <span class="n">u_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> 
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">u_x</span><span class="p">),</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">u_t</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">u_x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="nf">pow</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">σ</span><span class="sh">'</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="n">u_xx</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">u</span>     
  
    <span class="k">def</span> <span class="nf">_loss_func</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        
        <span class="n">u_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_u</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_u</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">t_u</span><span class="p">)</span>
        <span class="n">f_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_f</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_f</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">t_f</span><span class="p">)</span>
        <span class="n">loss_u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">u</span> <span class="o">-</span> <span class="n">u_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">loss_f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">f_pred</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_u</span> <span class="o">+</span> <span class="n">loss_f</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nb">iter</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span>
                <span class="sh">'</span><span class="s">Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">loss_u</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">loss_f</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ierr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="sh">'</span><span class="s">loss_u</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss_u</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="sh">'</span><span class="s">loss_f</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss_f</span><span class="p">.</span><span class="nf">item</span><span class="p">()}</span>    
        
        <span class="k">return</span> <span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>        
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_loss_func</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_net_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">f</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="pricing"/><category term="options"/><category term="stochastic-calculus"/><category term="machine-learning"/><summary type="html"><![CDATA[Solving the Black-Scholes PDE with physics inspired neural networks]]></summary></entry><entry><title type="html">Hedging Outside of Utopia</title><link href="https://sellersgaard.github.io/blog/2023/hedgeessays/" rel="alternate" type="text/html" title="Hedging Outside of Utopia"/><published>2023-12-17T11:00:00+00:00</published><updated>2023-12-17T11:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2023/hedgeessays</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2023/hedgeessays/"><![CDATA[<figure> <picture> <img src="/assets/img/hedge.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Traders Lost in the Hedge Maze</em></p> <p>I recently wrote <a href="https://github.com/SEllersgaard/Blog/blob/main/Hedge.ipynb">five short essays</a> on the subtle art of derivative hedging.</p> <p>Financial mathematics is often done under highly idealised assumptions, especially at the introductory level. This is unfortunate since students can leave academic programs with a somewhat disconnected understanding of the practical nature of financial engineering. In the referenced notebook I endeavour to walk a fine line between academia and industry, exploring one of the most fundamental aspects of derivatives trading - <em>hedging</em> - in five easy pieces. Specifically, I strive for a kind of quasi- (or pseudo-) realism, where simulated data is considered, but a subset of the original <a href="https://www.macroption.com/black-scholes-assumptions/">Black-Scholes-Merton assumptions</a> gradually are alleviated.</p> <ol> <li>In <strong>Delta Hedging at Discrete Intervals</strong> I explore the effects of hedging a finite number of times: what error do we accrue vis-a-vis hedging in a continuous time economy?</li> <li>In <strong>Gamma Hedging at Discrete Intervals</strong> I extend the previous essay by hedging the second order sensitivity to the underlying with another derivative. What impact does this have on the hedge error?</li> <li>In <strong>Hedging with Volatility Uncertainty</strong> I dismiss the idea that implied, realised, and hedging volatilities are all equal. What happens to the PnL of our hedge portfolio when these are allowed to be different?</li> <li>In <strong>Hedging with Transaction Costs</strong> I banish the frictionless economy while seeking a notion of ‘optimal hedging’.</li> <li>Finally, in <strong>Risk Minimising Hedges</strong> I consider how necessarily imperfect hedges nonetheless can be made as good as possible, focusing on the case of stochastic volatility.</li> </ol> <p>These are all known theoretical results, but <em>regrettably</em> they remain obscure to many who <em>ought to know</em>.<br/> My contribution here is purely expository. Also: you are unlikely to find this stuff coded up anywhere else, so here you go:</p> <blockquote> <p>“<em>To read the five essays please click <a href="https://github.com/SEllersgaard/Blog/blob/main/Hedge.ipynb">here</a>. The link takes you to a Jupyter Notebook hosted on Github.</em>”</p> </blockquote> <p>For a sample of the Notebook, I provide an abridged (code free) version of the first chapter below:</p> <h1 id="ch-1-delta-hedging-at-discrete-intervals">Ch. 1: Delta Hedging at Discrete Intervals</h1> <p>Black and Scholes famously demonstrated that the price process of a call option \(C_t = C_t(K,T)\) is perfectly replicable by dynamically adjusting a self-financing position in nothing but a risk-free bank account \(B_t\) and the underlying security \(S_t\). However, the conditions under which this is true are notoriously incredulous: for example, market are manifestly <em>not</em> free from transaction costs and <em>nobody</em> can adjust a portfolio continuously in time.</p> <p>So traders adjust their portfolios at discrete intervals \(\mathbb{T} = \{t_1, t_2, ..., t_n\}\), thereby inducing some measure of hedge error. How stark is the discrepancy? To answer this question, let us first consider where a discretely rebalanced hedge portfolio ends up vis-a-vis the terminal payoff under \(M\) different simulated stock paths. We consider initialising a hedge portfolio \(\Pi = B + \Delta S\) such that at time 0, the value is equivalent to the call option \(C_0\). For each \(t \in \mathbb{T}\) we re-balance the portfolio, adjusting the position in the stock to be that of the relevant Black-Scholes delta at the time. This rebalancing is done in a self-financing manner, meaning that the value of the portfolio <em>just after</em> the rebalancing, is identical to the value of the portfolio <em>just before</em> the rebalancing. Formally, the position in the bank at rebalancing time \(t_{i}^+\) must satisfy the equation</p> \[B_{t_{i}^-} + \Delta_{t_{i}^-} S_{t_{i}^-} = B_{t_{i}^+} + \Delta_{t_{i}^+} S_{t_{i}^+} \Longleftrightarrow B_{t_{i-1}}e^{r (t_i-t_{i-1})} + \Delta_{t_{i-1}} S_{t_{i}} = B_{t_i} + \Delta_{t_i} S_{t_{i}}.\] <p>Suppose we simulate \(M=500\) geometric Brownian motion paths with the following parameter specifications:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">S_0</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># spot
</span><span class="n">K</span> <span class="o">=</span> <span class="mi">110</span>  <span class="c1"># strike
</span><span class="n">τ</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># time to maturity
</span><span class="n">μ</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># mean drift
</span><span class="n">σ</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># volatility
</span><span class="n">r</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># risk free rate
</span></code></pre></div></div> <p>If we use these paths to form \(M\) self-financing portfolios rebalanced <em>once per day</em>, we can compare the terminal value of the hedge portfolio \(\Pi_T\) with the option pay-off \(C_T = \max\{S_T-K,0 \}\). Doing so, I get the plot below.</p> <p>Rejoice! The discrete hedge matches the option payoff to a good degree of accuracy, with the bulk of discrepancies lying around the at-the-money point (unsurprising: recall \(\Delta\) converges to \(\mathbb{1}\{S_T \geq K\}\) at maturity, meaning that it is easy to get the delta wrong around this point).</p> <figure> <picture> <img src="/assets/img/hedge1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>How badly does this deteriorate if we decrease the hedge frequency - e.g. by switching to a ‘once every 21 days’ rebalancing scheme? The figure below gives some intuition: the hedge portfolios are markedly more off-target, but not comically so.</p> <figure> <picture> <img src="/assets/img/hedge2.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To uncover the relationship between hedge frequencies and hedge errors, it is instructive to run this experiment for a range of hedge intervals, computing the standard deviation of the hedge error \((\Pi_T - C_T)\) in each case. In the plot below I consider hedging once every \(\{1, 3, 5, 7, 14, 21, 42, 63 \}\) days:</p> <figure> <picture> <img src="/assets/img/hedge3.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Based on this log-log plot we conclude that <strong>the hedge error scales as one over the square root of the number of hedges performed during the lifetime of the option</strong> (\(R^2 \approx 1\)). I.e. when you quadruple your hedge frequency you halve your hedge error.</p> <p>Can we show this formally? Yes! It’s slightly hairy, but the argument runs along the following lines: consider the hedge error \(\epsilon_{\delta t}\) that arises between an option and a replicating portfolio \(\Pi = B+\Delta S\) over some discrete interval \(\delta t\):</p> \[\begin{aligned} \epsilon_{\delta t} &amp; \equiv \delta C - \delta\Pi \\ &amp;=\delta C - \delta (B + \Delta S) \\ &amp;\approx \delta C - r B \delta t - \Delta \delta S \\ &amp;= \delta C - r (C-\Delta S) \delta t - \Delta \delta S. \end{aligned}\] <p>Now \(\delta S = \mu S \delta t + \sigma S \sqrt{\delta t} Z\), where \(Z \sim N(0,1)\), and by Itô \(\delta C \approx \theta \delta t + \Delta \delta S + \tfrac{1}{2} \Gamma (\delta S)^2\). Substituting these expression into the above, and ignoring terms \(O(\delta t^2)\) we obtain</p> \[\begin{aligned} \epsilon_{\delta t} &amp;\approx (\theta - rC + r \Delta S ) \delta t + \tfrac{1}{2} \Gamma \sigma^2 S^2 Z^2 \delta t \\ &amp;= \tfrac{1}{2} \Gamma \sigma^2 S^2 (Z^2 - 1) \delta t, \end{aligned}\] <p>where the second line has made use of the Black-Scholes PDE: \(rC = \theta + r\Delta S + \tfrac{1}{2} \sigma^2 S^2 \Gamma\). The cumulative hedge error from discrete hedging over the lifetime of the option is therefore</p> \[\epsilon \approx \sum_{i=1}^n \tfrac{1}{2} \Gamma_i \sigma^2 S_i^2 (Z_i^2 - 1) \delta t,\] <p>where \(\Gamma_i\) is understood to be the gamma at time \(t_i\) (ttm \(T-t_i\)) when spot is \(S_i\). By iterated expectations this has expectation 0 (recall \(\mathbb{E}[Z^2]=1\)). Meanwhile the variance of the hedge error is approximately</p> \[\mathbb{V}[\epsilon] \approx \mathbb{E} \left[ \sum_{i=1}^n \tfrac{1}{2} (\Gamma_i S_i^2)^2 (\sigma^2 \delta t)^2 \right]\] <p>since \(\mathbb{E}[(Z^2-1)^2] = \mathbb{E}[Z^4-2Z^2+1] = 3-2+1 = 2\). Evaluating this requires some effort. <a href="https://www.amazon.co.uk/Volatility-Smile-Wiley-Finance/dp/1118959167">Derman and Miller</a> provide (some) of the hard work (see eqns. (6.8) and (6.9)) if you are interested. Here, the upshot will do:</p> \[\mathbb{V}[\epsilon] \approx \sum_{i=1}^n \tfrac{1}{2} S_0^4 \Gamma_0^2 \sqrt{\frac{T^2}{T^2-t_i^2}} (\sigma^2 \delta t)^2 \approx \frac{\pi}{4} n (S_0^2 \Gamma_0 \sigma^2 \delta t)^2,\] <p>where \(n \equiv (T-t)/\delta t\). Finally, it is well known that gamma can be expressed as vega through \(S_0^2 \Gamma_0 \sigma (T-t) = \nu\). Using this fact, we arrive at</p> \[\mathbb{V}[\epsilon] \approx \frac{\pi}{4n} (\sigma \nu)^2 \Longrightarrow \text{std}(\epsilon) \approx \frac{\sigma}{\sqrt{n}} \nu\] <p>which is the \(\propto 1/\sqrt{n}\) relationship we wanted to show.</p> <h1 id="want-more">Want More?</h1> <p>To read all five essays <em>with</em> Python code, click <a href="https://github.com/SEllersgaard/Blog/blob/main/Hedge.ipynb">here</a>.</p>]]></content><author><name></name></author><category term="portfolio"/><category term="options"/><category term="trading"/><category term="volatility"/><summary type="html"><![CDATA[Five easy pieces on derivative hedging, bridging the gap between academia and industry]]></summary></entry><entry><title type="html">The SVI Model - A Tutorial</title><link href="https://sellersgaard.github.io/blog/2023/svi/" rel="alternate" type="text/html" title="The SVI Model - A Tutorial"/><published>2023-11-14T12:00:00+00:00</published><updated>2023-11-14T12:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2023/svi</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2023/svi/"><![CDATA[<figure> <picture> <img src="/assets/img/krishna.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>When a smile means the world: in the Bhagavata Purana, Krishna famously gives his foster mother a glimpse of his true essence (“Vishvarupa”) in a most unusual manner. At a more pedestrian level the ‘world’ is also baked into the volatility smile which codifies the market view of the (risk neutral) distributive properties of the underlying security.</em></p> <h3 id="the-svi-model-the-theoretical-minimum">The SVI model: The theoretical minimum.</h3> <p>The Stochastic Volatility Inspired (SVI) model is a parametric equation for the volatility smile designed by <a href="https://en.wikipedia.org/wiki/Jim_Gatheral">Jim Gatheral</a> during his time at Merrill Lynch in the late 1990s. Several highly readable theoretical expositions exist including <a href="https://mfe.baruch.cuny.edu/wp-content/uploads/2013/01/OsakaSVI2012.pdf">Gatheral</a> , <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2033323">Gatheral and Jacquier</a>, and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3543766">Ferhati</a>, which you are encouraged to consult. In this piece we provide a self-contained introduction to the subject, focussing on implementation: the theory is deliberately kept cursory. Note that I’ve only run this code ad hoc, and make no claims of it being production grade. If you find anything untoward, let me know.</p> <p>The core idea of the SVI can be summarised as follows: rather than fitting a global model of the volatility surface in “one go” the SVI model sets out to fit each smile individually (one expiry date at a time), often with careful constraints applied to ensure that all smiles preclude arbitrage. Specifically, for a given tenor \(\tau\), the SVI model postulates that the smile be modelled as</p> \[w(k,\tau) = a + b \left\{ \varrho (k-m) + \sqrt{(k-m)^2 + \sigma^2} \right\},\] <p>where \(w: \mathbb{R} \times [0,T] \mapsto \mathbb{R}^+\) is the <em>total variance</em>, defined as \(w(k,\tau) \equiv \tau \sigma_{BS}^2(k,\tau)\) (the time scaled Black Scholes implied vol <em>squared</em>) and \(k\) is the <em>log forward moneyness</em>, defined as \(\ln(K/F_\tau)\), where \(K\) and \(F_\tau\) are the strike and forward prices. For ease of notation we have suppressed the \(\tau\) dependence on the parameters \(\chi \equiv \{a,b,\varrho, m, \sigma\}\), but keep this in mind. Roughly speaking, \(a \in \mathbb{R}\) controls the overall variance level (vertical translations); \(b \in \mathbb{R}^+\) controls the slope of the wings; \(\varrho \in (-1,1)\) controls the counter-clockwise rotation of the smile; \(m \in \mathbb{R}\) the vertical translations, and \(\sigma \in \mathbb{R}^+\) the level of curvature.</p> <p>Further constraints must be enforced upon the parameters to ensure sensible results. For starters Gatheral lists the requirement that \(a+b \sigma \sqrt{1-\varrho^2} \geq 0\) for a non-negative total variance. More subtle constraints arise from eliminating arbitrage. E.g. on the intra-smile level, the absence of <em>butterfly arbitrage</em> entails that one cannot go long a <a href="https://en.wikipedia.org/wiki/Butterfly_(options)">fly (option structure)</a> without paying for the pleasure. (A fly has a pay-off structure which is everywhere non-negative. You don’t get this sort of optionality for free). By <a href="https://quantpy.com.au/stochastic-calculus/breeden-litzenberger-formula-for-risk-neutral-densities/">Breeden Litzenberger</a> this turns out to be equivalent to requiring that the risk neutral density function is everywhere non-negative (a very reasonable disposition indeed). A tedious argument shows that this amounts to requiring that \(g(k) \geq 0\), \(\forall k\) where we have defined</p> \[g(k) \equiv \left( 1-\frac{kw'(k)}{2w(k)} \right)^2 - \frac{w'(k)^2}{4} \left( \frac{1}{w(k)} + \frac{1}{4}\right) + \frac{w''(k)}{2},\] <p>… which hardly is the sort of expression you want to throw at an optimisation problem. Fortunately, based on Roger Lee’s moment formula, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3543766">Ferhati</a> shows that for the SVI we can get away with requiring something considerably simpler, viz.</p> <ul> <li>\((a-mb(\varrho+1))(4-a+mb(\varrho+1)) &gt; b^2(\varrho+1)^2\).</li> <li>\((a-mb(\varrho-1))(4-a+mb(\varrho-1)) &gt; b^2(\varrho-1)^2\).</li> <li>\(0&lt;b^2(\varrho+1)^2&lt;4\).</li> <li>\(0&lt;b^2(\varrho-1)^2&lt;4\).</li> </ul> <p>In the implementation below, this is what we run with.</p> <p>Alright so the individual smiles can be made arb free, but what about when viewed collectively? It turns out that unless we require that call option prices are monotonically increasing as a function of time to maturity, the surface will also admit <em>calendar arbitrage</em>. To see this, imagine \(C(k,\tau_1) &gt; C(k,\tau_2)\) where \(\tau_1 &lt; \tau_2\), and that we sell short the first option and go long the second (a <a href="https://en.wikipedia.org/wiki/Calendar_spread">calendar spreads</a>), thus pocketing an initial premium. If the front option expires worthless, we have zero downside. On the other hand, should it expire in-the-money, we can cover our position by shorting the underlying at the prevailing price level: regardless of where the price ends up at \(\tau_2\) we make money (convince yourself this is the case). Altogether, this situation is indeed an arbitrage. To avoid it we can (equivalently) require that the total variance function \(w(k,\tau)\) is monotonically increasing as a function of time to maturity, \(\forall k: \partial_\tau w(k,\tau) \geq 0\), - or in graphical terms - that there are no crossed curves (smiles) in a total variance plot. Vis-à-vis the butterfly conditions above, this constraint is much less nimble. As you will see below, we fit total variance smiles sequentially starting with the nearest tenor, requiring that each new smile being fit is bounded from below by the most recently fitted smile. The drawback of this methodology is that it grants a distinct ontological privilege to the first smile in the batch: with often poor liquidity provided for ultra near-dated securities, this can potentially lead to sub-optimal calibrations.</p> <p>Altogether a volatility surface is said to be void of <a href="https://mfe.baruch.cuny.edu/wp-content/uploads/2013/04/BloombergSVI2013.pdf">static arbitrage</a> if it rules out both butterfly and calendar arbitrage. This is the benchmark against which all volatility models ultimately must be assessed (if the goal is simply to find the “best fit”, the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a> suggests that a Neural Network solution will get you there in just a few lines of code).</p> <p>Before we move on to the implementation, a note on nomenclature: what exactly is ‘stochastic volatility inspired’ about the SVI model? The classical <a href="https://en.wikipedia.org/wiki/Heston_model">Heston model</a>, much adored by academics, is known to provide rapid arbitrage free surface calibrations, albeit at the cost of considerable inaccuracy, particularly for shorter maturities (the model only has five parameters). The neat part about the SVI model is that it converges asymptotically to the Heston model for large maturities, i.e.</p> \[\lim_{\tau \rightarrow \infty} \sigma_{\text{SVI}}^2(k) = \sigma_{\text{Heston}}^2(k), \forall k \in \mathbb{R}.\] <p>The model is therefore Heston <em>inspired</em>, but does not assume the global verisimilitude thereof. By having five parameters <em>per smile</em> as opposed to <em>per surface</em> we can unsurprisingly model the volatility surface with considerably greater accuracy.</p> <h3 id="coding-it-up">Coding it up</h3> <p>In the code snippet below I provide a Python implementation of the SVI model. The class takes as its input a pandas dataframe containing the volatility surface on tabular form. The frame must as a minimum contain the following five columns: the implied volatility (‘IV’) in percentage terms: (float), the strike price (‘Strike’): (float), the expiry date (‘Date’): (pd.Timestamp) the time to maturity (‘Tau’) in years: (float), and the forward price (‘F’): (float). As with all numerical routines, make sure you put in the proper amount of data cleaning effort: restricting the space of options to highly liquid ones: out-of-the-money puts and calls with absolute deltas greater than 0.05 is probably a good place to start.</p> <p>The model is calibrated using the .fit method. For a given maturity \(\tau\) this is achieved by minimising the quadratic cost function</p> \[\mathfrak{L}(\chi, \tau) = \sum_{i \in \mathbb{S}_\tau} (w_{SVI}(k_i, \tau) - w_{obs}(k_i, \tau))^2,\] <p>where \(\mathbb{S}_\tau\) is the set of coordinates making up a given smile. Smiles are fit in the ordered fashion \(\tau_1 &lt; \tau_2 &lt; ... &lt; \tau_n\), and we deploy SciPy’s implementation of the <a href="https://docs.scipy.org/doc/scipy/tutorial/optimize.html#sequential-least-squares-programming-slsqp-algorithm-method-slsqp">Sequantial Least SQuares Programming (SLSQP) algorithm</a> to handle the no-arbitrage constraints. To speed up the rate of convergence, I supply the minimiser with the gradient (Jacobian) of the objective function as well as the constraints: tedious expressions, provided in the various _jac functions below.</p> <p>Although not used for calibration purposes, the code also includes the risk neutral density function</p> \[q(k) = \frac{g(k)}{\sqrt{2\pi w(k)}} \exp \left \{ \frac{d_{-}^2}{2} \right \},\] <p>where \(d_{-} \equiv -k/\sqrt{w} - \sqrt{w}/2\). The class can thus readily be amended for pricing purposes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span><span class="p">,</span> <span class="n">minimize</span><span class="p">,</span> <span class="n">Bounds</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="k">class</span> <span class="nc">SVIModel</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This class fits Gatheral</span><span class="sh">'</span><span class="s">s Stochastic Volatility Inspired (SVI) model to a pandas dataframe of 
    implied volatility data. The pandas dataframe must contain the following columns: 
    
    i. The implied volatility (</span><span class="sh">'</span><span class="s">IV</span><span class="sh">'</span><span class="s">) in %: (float64)
    ii. The strike price (</span><span class="sh">'</span><span class="s">Strike</span><span class="sh">'</span><span class="s">): (float64)
    iii. The expiry date (</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="s">): (pd.Timestamp)
    iv. The time to maturity (</span><span class="sh">'</span><span class="s">Tau</span><span class="sh">'</span><span class="s">) in years: (float64)
    v. The forward price (</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="s">): (float64)
    
    The vol surface is fit smile-by-smile using a Sequential Least SQuares Programming optimizer which has
    the option of preventing static arbitrage (butterfly and calendar arbitrage).
    To perform the calibration, call the fit method.
    The calibrated parameters are saved in the dictionary </span><span class="sh">'</span><span class="s">param_dic</span><span class="sh">'</span><span class="s">, but can also be returned as a pandas dataframe.
    
    Simon Ellersgaard Nielsen, 2023-11-12
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">min_fit</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        df: Volatility dataframe. Must contain [</span><span class="sh">'</span><span class="s">IV</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="s">Strike</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="s">Tau</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="s">]
        min_fit: The minimum number of observations per smile required.
        </span><span class="sh">"""</span>
        
        <span class="k">assert</span> <span class="nf">type</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">==</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span>
        <span class="k">assert</span> <span class="sh">'</span><span class="s">IV</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span>
        <span class="k">assert</span> <span class="sh">'</span><span class="s">Strike</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span>
        <span class="k">assert</span> <span class="sh">'</span><span class="s">Date</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span>
        <span class="k">assert</span> <span class="sh">'</span><span class="s">Tau</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span>
        <span class="k">assert</span> <span class="sh">'</span><span class="s">F</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span>
        
        <span class="n">dfv</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        
        <span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">TV</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">Tau</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="p">((</span><span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">IV</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="mf">100.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">dfvs</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">IV</span><span class="sh">'</span><span class="p">].</span><span class="nf">count</span><span class="p">()</span> 
        <span class="n">dfvs</span> <span class="o">=</span> <span class="n">dfvs</span><span class="p">[</span><span class="n">dfvs</span><span class="o">&gt;=</span><span class="n">min_fit</span><span class="p">]</span>
        <span class="n">dfv</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">[</span><span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">].</span><span class="nf">isin</span><span class="p">(</span><span class="n">dfvs</span><span class="p">.</span><span class="n">index</span><span class="p">)]</span>
        
        <span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">LogM</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">Strike</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">dfv</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">([</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Strike</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">Tau</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">dfv_dic</span> <span class="o">=</span> <span class="p">{</span><span class="n">t</span><span class="p">:</span> <span class="n">dfv</span><span class="p">[</span><span class="n">dfv</span><span class="p">[</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lbl</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ρ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">m</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">σ</span><span class="sh">'</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">no_butterfly</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">no_calendar</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">plotsvi</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Fits SVI model smile-by-smile to the data. If no no-arbitrage constraints are enforced the curves are fit using
        SciPys curve_fit. If no-arbitrage is required we use SciPy</span><span class="sh">'</span><span class="s">s sequential least squares minimization.
        </span><span class="sh">"""</span>
        
        <span class="n">ϵ</span> <span class="o">=</span> <span class="mf">1e-6</span>
        <span class="n">bnd</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">'</span><span class="s">bnd</span><span class="sh">'</span><span class="p">,</span> <span class="p">([</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="o">+</span><span class="n">ϵ</span><span class="p">,</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">ϵ</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">]))</span>
        <span class="n">p0</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">'</span><span class="s">p0</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">])</span>       
        <span class="n">self</span><span class="p">.</span><span class="n">no_butterfly</span> <span class="o">=</span> <span class="n">no_butterfly</span>
        <span class="n">self</span><span class="p">.</span><span class="n">no_calendar</span> <span class="o">=</span> <span class="n">no_calendar</span>
        
        <span class="c1"># Loop over the individual smiles
</span>        <span class="n">self</span><span class="p">.</span><span class="n">param_dic</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">):</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">ti</span> <span class="o">=</span> <span class="n">ti</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">xdata</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dfv_dic</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="sh">'</span><span class="s">LogM</span><span class="sh">'</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ydata</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dfv_dic</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="sh">'</span><span class="s">TV</span><span class="sh">'</span><span class="p">]</span>
            
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="n">no_butterfly</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="ow">not</span> <span class="n">no_calendar</span><span class="p">):</span>
                <span class="n">maxfev</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">'</span><span class="s">maxfev</span><span class="sh">'</span><span class="p">,</span><span class="mf">1e6</span><span class="p">)</span>
                <span class="n">popt</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">curve_fit</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">svi</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">xdata</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">ydata</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">svi_jac</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span>  <span class="n">bounds</span><span class="o">=</span><span class="n">bnd</span><span class="p">,</span> <span class="n">maxfev</span><span class="o">=</span><span class="n">maxfev</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ineq_cons</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_no_arbitrage</span><span class="p">()</span>
                <span class="n">tol</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">'</span><span class="s">tol</span><span class="sh">'</span><span class="p">,</span> <span class="mf">1e-50</span><span class="p">)</span>
                <span class="n">res</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">svi_mse</span><span class="p">,</span> 
                               <span class="n">p0</span><span class="p">,</span> 
                               <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">xdata</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">ydata</span><span class="p">),</span> 
                               <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">SLSQP</span><span class="sh">'</span><span class="p">,</span> 
                               <span class="n">jac</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">svi_mse_jac</span><span class="p">,</span>
                               <span class="n">constraints</span><span class="o">=</span><span class="n">ineq_cons</span><span class="p">,</span>  
                               <span class="n">bounds</span><span class="o">=</span><span class="nc">Bounds</span><span class="p">(</span><span class="n">bnd</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">bnd</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> 
                               <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                               <span class="n">options</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="n">popt</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">x</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">param_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lbl</span><span class="p">,</span> <span class="n">popt</span><span class="p">))</span> 
    
            <span class="k">if</span> <span class="n">plotsvi</span><span class="p">:</span>
                <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">xdata</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">ydata</span><span class="p">)</span>
                <span class="n">yest</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">popt</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">xdata</span><span class="p">,</span><span class="n">yest</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="nf">from_dict</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">param_dic</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">index</span><span class="sh">'</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">_no_arbitrage</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        No arbitrage constraints on the SVI fit
        </span><span class="sh">"""</span>
        
        <span class="n">ineq_cons</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">no_butterfly</span><span class="p">:</span>  
            <span class="n">ineq_cons</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">ineq</span><span class="sh">'</span><span class="p">,</span>
                 <span class="sh">'</span><span class="s">fun</span><span class="sh">'</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">svi_butterfly</span><span class="p">,</span>
                 <span class="sh">'</span><span class="s">jac</span><span class="sh">'</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">svi_butterfly_jac</span><span class="p">})</span>
            
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">no_calendar</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ti</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">xv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">xdata</span><span class="p">.</span><span class="n">values</span>
                <span class="n">xv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">xv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">xv</span><span class="p">),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">xv</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>
                <span class="n">pv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">param_dic</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">ti</span><span class="o">-</span><span class="mi">1</span><span class="p">]][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">lbl</span><span class="p">])</span>
                <span class="n">ineq_cons</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
                    <span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">ineq</span><span class="sh">'</span><span class="p">,</span>
                    <span class="sh">'</span><span class="s">fun</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">svi_calendar</span><span class="p">,</span>
                    <span class="sh">'</span><span class="s">jac</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">svi_calendar_jac</span><span class="p">,</span>
                    <span class="sh">'</span><span class="s">args</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="n">pv</span><span class="p">,</span> <span class="n">xv</span><span class="p">),</span>
                <span class="p">})</span>
            
        <span class="k">return</span> <span class="n">ineq_cons</span>
        
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">svi</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">ρ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">σ</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        SVI parameterisation of the total variance curve 
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span> <span class="n">ρ</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="p">))</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">dsvi</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">ρ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">σ</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        d(SVI)/dk 
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">*</span><span class="n">ρ</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
        
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">d2svi</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">ρ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">σ</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        d^2(SVI)/dk^2 
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">*</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">q_density</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">ρ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">σ</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span> 
        <span class="sh">"""</span><span class="s">
        Gatheral</span><span class="sh">'</span><span class="s">s risk neutral density function
        </span><span class="sh">"""</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">ρ</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">σ</span><span class="p">])</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dsvi</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="n">d2w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">d2svi</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="n">k</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">k</span><span class="o">*</span><span class="n">dw</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">w</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">0.25</span><span class="o">*</span><span class="p">((</span><span class="n">dw</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">w</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">d2w</span>
        <span class="k">return</span> <span class="n">g</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">svi_jac</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">ρ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">σ</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Jacobian of the SVI parameterisation
        </span><span class="sh">"""</span>
        <span class="n">dsda</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
        <span class="n">dsdb</span> <span class="o">=</span> <span class="n">ρ</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">dsdρ</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span>
        <span class="n">dsdm</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">ρ</span><span class="o">+</span><span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">dsdσ</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="n">σ</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">dsda</span><span class="p">,</span><span class="n">dsdb</span><span class="p">,</span><span class="n">dsdρ</span><span class="p">,</span><span class="n">dsdm</span><span class="p">,</span><span class="n">dsdσ</span><span class="p">]).</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">svi_mse</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">xdata</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">ydata</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Sum of squared errors of the SVI model
        </span><span class="sh">"""</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">ydata</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">svi_mse_jac</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">xdata</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">ydata</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Jacobian of the sum of squared errors
        </span><span class="sh">"""</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="n">jac</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi_jac</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">ydata</span><span class="p">).</span><span class="n">T</span><span class="p">.</span><span class="n">values</span><span class="o">*</span><span class="p">(</span><span class="n">jac</span><span class="p">).</span><span class="n">T</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">svi_butterfly</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        SVI butterfly arbitrage constraints (all must be &gt;= 0)
        </span><span class="sh">"""</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">ρ</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">σ</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">c1</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">m</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">c2</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">m</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">c3</span> <span class="o">=</span> <span class="mi">4</span><span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">c4</span> <span class="o">=</span> <span class="mi">4</span><span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">c1</span><span class="p">,</span><span class="n">c2</span><span class="p">,</span><span class="n">c3</span><span class="p">,</span><span class="n">c4</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">svi_butterfly_jac</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Jacobian of SVI butterfly constraints
        </span><span class="sh">"""</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">ρ</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">σ</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">dc1da</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">a</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span>
        <span class="n">dc1db</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">dc1dρ</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">ρ</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">dc1dm</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">dc2da</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">a</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span>
        <span class="n">dc2db</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">dc2dρ</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">ρ</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">dc2dm</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">dc3db</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">dc3dρ</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">ρ</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">dc4db</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">ρ</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">dc4dρ</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">ρ</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">dc1dσ</span> <span class="o">=</span> <span class="n">dc2dσ</span> <span class="o">=</span> <span class="n">dc3da</span> <span class="o">=</span> <span class="n">dc3dm</span> <span class="o">=</span> <span class="n">dc3dσ</span> <span class="o">=</span> <span class="n">dc4da</span> <span class="o">=</span> <span class="n">dc4dm</span> <span class="o">=</span> <span class="n">dc4dσ</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">dc1da</span><span class="p">,</span> <span class="n">dc1db</span><span class="p">,</span> <span class="n">dc1dρ</span><span class="p">,</span> <span class="n">dc1dm</span><span class="p">,</span> <span class="n">dc1dσ</span><span class="p">],</span>
                         <span class="p">[</span><span class="n">dc2da</span><span class="p">,</span> <span class="n">dc2db</span><span class="p">,</span> <span class="n">dc2dρ</span><span class="p">,</span> <span class="n">dc2dm</span><span class="p">,</span> <span class="n">dc2dσ</span><span class="p">],</span>
                         <span class="p">[</span><span class="n">dc3da</span><span class="p">,</span> <span class="n">dc3db</span><span class="p">,</span> <span class="n">dc3dρ</span><span class="p">,</span> <span class="n">dc3dm</span><span class="p">,</span> <span class="n">dc3dσ</span><span class="p">],</span>
                         <span class="p">[</span><span class="n">dc4da</span><span class="p">,</span> <span class="n">dc4db</span><span class="p">,</span> <span class="n">dc4dρ</span><span class="p">,</span> <span class="n">dc4dm</span><span class="p">,</span> <span class="n">dc4dσ</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">svi_calendar</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">params_old</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        SVI calendar arbitrage constraint (must be &gt;= 0)
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">params_old</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">svi_calendar_jac</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">params_old</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Jacobian of SVI calendar constraint 
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">svi_jac</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
        
</code></pre></div></div> <h3 id="an-example-from-crypto">An Example From Crypto</h3> <p>Derivatives trading is still a comparatively nascent market in the digital assets space. The largest crypto options exchange, <a href="https://www.deribit.com/options/BTC">Deribit</a>, allows us with comparative ease to retrieve bid-ask quotes for puts and calls on Bitcoin across a range of strikes and times-to-maturity. The quoting convention is more akin to equity than FX in the sense that strikes are listed in dollar terms, rather than delta. Note though that prices are quoted in units of crypto, reflecting the fact that the options are <a href="https://www.researchgate.net/publication/353478878_Inverse_Options_in_a_Black-Scholes_World">inverse</a> (i.e. have a crypto denominated payoff). Furthermore, crypto being crypto, the implied volatility levels are on average considerably higher (\(\sim\)5x) than the TradFi market.</p> <p>In the example below, I have plotted a bitcoin vol surface which was sampled on 2023-02-16. The lines reflect the SVI fit with ‘no static arbitrage’ constraints enforced. Clearly, the SVI model provides an excellent fit to the volatility surface.</p> <div class="l-page"> <iframe src="/assets/plotly/surface.html" frameborder="0" scrolling="no" height="700px" width="105%" style="border: 1px dashed grey;"></iframe> </div> <p>The above graph was generated using the plotly code below. Being obsessed with certain stylistic elements I admit this is on the longer side:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>

<span class="k">class</span> <span class="nc">SVIPlot</span><span class="p">(</span><span class="n">SVIModel</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">allsmiles</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sv</span><span class="p">:</span> <span class="nf">type</span><span class="p">(</span><span class="n">SVIModel</span><span class="p">)):</span>
        <span class="sh">"""</span><span class="s">
        Plots all volatility smiles in a single 3d figure (data and fit)
        </span><span class="sh">"""</span>

        <span class="n">dfv_dic</span><span class="p">,</span> <span class="n">param_dic</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">sv</span><span class="p">.</span><span class="n">dfv_dic</span><span class="p">,</span> <span class="n">sv</span><span class="p">.</span><span class="n">param_dic</span><span class="p">,</span> <span class="n">sv</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sv</span><span class="p">.</span><span class="n">tau</span>

        <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">dfv_dic</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="sh">'</span><span class="s">LogM</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">dfv_dic</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="sh">'</span><span class="s">IV</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">dfv_dic</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

            <span class="nf">print</span><span class="p">(</span><span class="n">param_dic</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>

            <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span>  <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">1.1</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">x1</span><span class="o">-</span><span class="n">x0</span><span class="p">)</span><span class="o">/</span><span class="mi">200</span>
            <span class="n">xnew</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">dx</span><span class="p">)</span>
            <span class="n">znew</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">svi</span><span class="p">(</span><span class="n">xnew</span><span class="p">,</span> <span class="o">**</span><span class="n">param_dic</span><span class="p">[</span><span class="n">t</span><span class="p">])</span><span class="o">/</span><span class="n">tau</span><span class="p">[</span><span class="n">ti</span><span class="p">])</span>
            <span class="c1">#znew = 100*np.sqrt(self.svi(xnew, param_dic[t]['a'], param_dic[t]['b'], param_dic[t]['ρ'], param_dic[t]['m'], param_dic[t]['σ'])/tau[ti])
</span>            <span class="n">ynew</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">t</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">xnew</span><span class="p">))</span>
            <span class="c1">#ynew[-1] += timedelta(seconds=1)
</span>
            <span class="c1"># See https://plotly.com/python/legend/#grouped-legend-items
</span>            <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scatter3d</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">markers</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d</span><span class="sh">"</span><span class="p">),</span>
                <span class="n">legendgroup</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
                    <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">Black</span><span class="sh">'</span><span class="p">,</span>
                    <span class="c1">#colorscale='PuRd',
</span>                <span class="p">)))</span>
            <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scatter3d</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">xnew</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">ynew</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">znew</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">lines</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d</span><span class="sh">"</span><span class="p">),</span> <span class="n">legendgroup</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">ti</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">showlegend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="p">))</span>

        <span class="n">fig</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_change_camera</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

        <span class="n">fig</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_add_onoff</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

        <span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Volatility Surface</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">.</span><span class="nf">update_scenes</span><span class="p">(</span><span class="n">xaxis_title_text</span><span class="o">=</span><span class="sh">'</span><span class="s">Log Moneyness</span><span class="sh">'</span><span class="p">,</span>
                          <span class="n">yaxis_title_text</span><span class="o">=</span><span class="sh">'</span><span class="s">Time</span><span class="sh">'</span><span class="p">,</span>
                          <span class="n">zaxis_title_text</span><span class="o">=</span><span class="sh">'</span><span class="s">Implied Volatility</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">fig</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_change_camera</span><span class="p">(</span><span class="n">fig</span><span class="p">:</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">())</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">():</span>
        <span class="sh">"""</span><span class="s">
        Adjusts initial view of vol surface
        </span><span class="sh">"""</span>

        <span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span>
            <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span>
            <span class="n">height</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span>
            <span class="n">autosize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">scene</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
                <span class="n">camera</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
                    <span class="n">up</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                    <span class="n">eye</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">1.25</span><span class="p">,</span> <span class="n">y</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="n">aspectratio</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span> <span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mf">0.7</span> <span class="p">),</span>
                <span class="n">aspectmode</span> <span class="o">=</span> <span class="sh">'</span><span class="s">manual</span><span class="sh">'</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">fig</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_add_onoff</span><span class="p">(</span><span class="n">fig</span><span class="p">:</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">())</span> <span class="o">-&gt;</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">():</span>
        <span class="sh">"""</span><span class="s">
        Add select/deselect all buttons to plot
        </span><span class="sh">"""</span>

        <span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span><span class="nf">dict</span><span class="p">(</span><span class="n">updatemenus</span><span class="o">=</span><span class="p">[</span>
            <span class="nf">dict</span><span class="p">(</span>
                <span class="nb">type</span> <span class="o">=</span> <span class="sh">"</span><span class="s">buttons</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">direction</span> <span class="o">=</span> <span class="sh">"</span><span class="s">left</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">buttons</span><span class="o">=</span><span class="nf">list</span><span class="p">([</span>
                    <span class="nf">dict</span><span class="p">(</span>
                        <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">visible</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">legendonly</span><span class="sh">"</span><span class="p">],</span>
                        <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Deselect All</span><span class="sh">"</span><span class="p">,</span>
                        <span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">restyle</span><span class="sh">"</span>
                    <span class="p">),</span>
                    <span class="nf">dict</span><span class="p">(</span>
                        <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">visible</span><span class="sh">"</span><span class="p">,</span> <span class="bp">True</span><span class="p">],</span>
                        <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Select All</span><span class="sh">"</span><span class="p">,</span>
                        <span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">restyle</span><span class="sh">"</span>
                    <span class="p">)</span>
                <span class="p">]),</span>
                <span class="n">pad</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="sh">"</span><span class="s">t</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>
                <span class="n">showactive</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">xanchor</span><span class="o">=</span><span class="sh">"</span><span class="s">right</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span>
                <span class="n">yanchor</span><span class="o">=</span><span class="sh">"</span><span class="s">top</span><span class="sh">"</span>
            <span class="p">),</span>
        <span class="p">]</span>
        <span class="p">))</span>
        <span class="k">return</span> <span class="n">fig</span>
</code></pre></div></div> <p>And that pretty much wraps it up. As a parting thought, it is interesting to consider whether the SVI surface calibration can be simplified: in particular, can we fit the entire surface in one go, e.g. by postulating explicit \(\tau\)-dependent expressions for the SVI parameters? The advantage of doing so would go beyond pure parsimony: in particular, it would also do away with the non-trivial issue of how one should interpolate between tenors. I’m not the first person to ponder this problem: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1779463">Gurrieri</a>, for example, takes a decent shot at postulating such expressions. As always, the devil is in the no-arbitrage detail: Gurrieri’s model is arb free, but only under certain tedious constraints. Whether this ultimately is a successful approach remains to be seen.</p> <p>Below I have plotted the calibrated parameters for various times to maturity. On a first inspection some of them seem more prone towards simple parametric fits than others. Are there universal laws for how these parameters should behave? How stable is the SVI fit to perturbations in the parameters? Well, these are open questions. You tell me.</p> <figure> <picture> <img src="/assets/img/svi_param2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="modelling"/><category term="volatility"/><category term="crypto"/><category term="options"/><summary type="html"><![CDATA[A Pythonesque implementation of Gatheral's volatility smile model]]></summary></entry><entry><title type="html">Model Multiplicity</title><link href="https://sellersgaard.github.io/blog/2023/modeluncertain/" rel="alternate" type="text/html" title="Model Multiplicity"/><published>2023-04-04T12:00:00+00:00</published><updated>2023-04-04T12:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2023/modeluncertain</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2023/modeluncertain/"><![CDATA[<figure> <picture> <img src="/assets/img/uncertain2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Probabilistic uncertainty as interpreted by DALL-E 2</em></p> <h3 id="bayesian-model-averaging">Bayesian model averaging</h3> <p>A well-known aphorism attributed to <a href="https://en.wikipedia.org/wiki/George_E._P._Box">George Box</a> states that “<em>all models are wrong, but some are useful</em>”. This observation rings particularly true in systematic investing, where the price action governed by millions of non-cooperating players, ultimately is explained through some sort of reductionist supervised learning problem. Nobody would seriously entertain the view that these models are <em>fundamental</em> or <em>expressively adequate</em>, yet it is common practice to let just a single model guide the trading process based on some performance metric like the out-of-sample mean-squared error. This is potentially troubling given that a less accurate model need not be universally inferior in all of its predictions. Indeed, it is conceivable that some biases cancel each other out if only we marry the predictions made by various models (see e.g. the work by <a href="https://en.wikipedia.org/wiki/Jennifer_A._Hoeting">Jennifer Hoeting</a>). This begs the question: how exactly do we combine said predictions? Intuitively, while simple linear averaging might work, the thought of attributing equal significance to models regardless of their level of performance is clearly distasteful. A somewhat subtler approach would be to weigh a prediction by the evidence of the model, and this is precisely what <em>Bayesian model averaging</em> sets out to do. In this piece I will provide an exegesis of this philosophy, and lay out what I consider to be serious obstacles and how to potentially overcome them.</p> <p>Let \(\boldsymbol{y}=(y_1, y_2, ..., y_N)^\intercal \in \mathbb{R}^{N}\) be a vector of data observations, which we desire to model. Furthermore, suppose we have \(K\) competing models \(\mathbb{M} = \{M_1, M_2, ..., M_K \}\) for \(\boldsymbol{y}\), each of which is characterised by some specific vector of parameters: \(\boldsymbol{\theta}_i = (\theta_{i,1},\theta_{i,2}, ..., \theta_{i,T_i})^\intercal \in \boldsymbol{\Theta}_i \subseteq \mathbb{R}^{T_i}\). The candidate models could be <a href="https://www.theanalysisfactor.com/what-are-nested-models/">nested</a> within the same super-model (e.g. all possible subsets of a multivariate linear regression), although this is <em>not</em> a requirement.</p> <p>Based on observable evidence, what is the probability of any given model? According to <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a> it follows that \(\forall i\):</p> <p>\begin{equation}\label{bayes} p(M_i | \boldsymbol{y} ) = \frac{p(\boldsymbol{y}| M_i) p(M_i) }{\sum_{j=1}^K p(\boldsymbol{y}| M_j) p(M_j)}, \end{equation}</p> <p>where \(p(\boldsymbol{y} \vert M_i)\) can be written as</p> <p>\begin{equation}\label{bayes2} p(\boldsymbol{y}| M_i) = \int_{\boldsymbol{\Theta}_i} p(\boldsymbol{y}| \boldsymbol{\theta}_i, M_i) p(\boldsymbol{\theta}_i | M_i) d\boldsymbol{\theta}_i. <br/> \end{equation}</p> <p>Here, the probability \(p(M_i)\) signifies our credence in model \(M_i\) before being presented with any available data. Obviously, for any given model, this is an entirely subjective matter, although a popular choice unsurprisingly is that of <a href="https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors">uniformity</a>: \(\forall i: p(M_i)=1/K\). In a similar vein, the probability \(p(\boldsymbol{\theta}_i | M_i)\) codifies our prior beliefs about how the parameters of model \(M_i\) are distributed, assuming the correctness of that model: again, a matter inherently subjective in nature. While people anchored in the frequentists’ paradigm might find the concept of subjective priors disturbingly nebulous, let me assure you that the importance of priors often is <em>over-stated</em>. Specifically, for sufficiently large \(N\) (no. of observations), sufficiently diffuse priors will be overshadowed by the evidence leading to approximately identical posteriors. The important point is that equation \eqref{bayes} provides a coherent framework for going from no empirical evidence, to looking at the data, to ultimately assigning an evidence based probability score to a given model.</p> <p>Now we can use our collection of posterior probabilities over the space of models \(\mathbb{M}\) to get a sense of “model free” (strictly speaking: model weighted) probabilities: e.g. if \(\Delta\) is some quantity of interest then \(p(\Delta | \boldsymbol{y}) = \sum_{j=1}^K p(\Delta | M_j, \boldsymbol{y}) p(M_j | \boldsymbol{y}).\) E.g. if we are in the business of forecasting, we can get a model-averaged expectation of the next observation using the equation:</p> <p>\begin{equation}\label{expectation} \mathbb{E}[y^* | \boldsymbol{y}] = \sum_{j=1}^K \mathbb{E}[y^* | \boldsymbol{y}, M_j] p(M_j | \boldsymbol{y}).<br/> \end{equation}</p> <p>Great, so should we start throwing everything but the kitchen sink when modelling our data? Well, not quite…</p> <h3 id="the-schwarz-approximation">The Schwarz approximation</h3> <p>There’s a rather glaring issue with \eqref{bayes} having to do with computability. For starters, the space of possible models quickly grows almost unimaginably large. For instance, the number of possible linear regressions you can run with 50 input features is well over a quadrillion (\(10^{15}\)). Secondly, finding explicit expression for the posterior distribution may often prove onerous if not downright impossible: instead, computer-intensive numerical methods such as <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov Chain Monte Carlo</a> (MCMC) must be called up. If this in turn must be run repeatedly in a back-testing system, the computational challenges quickly become insurmountable.</p> <p>Neither issue is trivially resolved, but there are a few tricks of the trade commonly employed. E.g. drastic reductions in the model space cardinality are quickly accomplished by (a) excluding models which predict the data far less well than the best model, and (b) throwing out complex models that receive less support from the data, than simpler models [<a href="https://www.jstor.org/stable/2676803">Hoeting et al.</a>]. Meanwhile, to circumvent the MCMC issue, we may follow Gideon Schwarz in approximating the posterior distribution using <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace’s method</a>.</p> <p>The basic idea is as follows<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>: suppose we take a flat prior on \(\boldsymbol{\Theta}_i\). Furthermore, let \(\ell(\boldsymbol{\theta}) \equiv \log(p(\boldsymbol{y}| \boldsymbol{\theta}_i, M_i) )\) denote the log-likelihood, \(\bar{\ell}\) the mean log-likelihood, and let \(\hat{\boldsymbol{\theta}}_i\) be the maximum likelihood estimator (MLE), then \eqref{bayes2} can be written as</p> \[\begin{aligned} p(\boldsymbol{y} | M_i) &amp;\propto \int_{\boldsymbol{\Theta}_i} e^{N \bar{\ell}(\boldsymbol{\theta}_i)} d\boldsymbol{\theta}_i \\ &amp; \approx e^{\ell(\hat{\boldsymbol{\theta}}_i)} \int_{\boldsymbol{\Theta}_i} e^{-\tfrac{1}{2} (\boldsymbol{\theta}_i - \hat{\boldsymbol{\theta}}_i)^\intercal N\frac{\partial^2 \bar{\ell}(\hat{\boldsymbol{\theta}}_i) }{\partial \boldsymbol{\theta}_i \partial \boldsymbol{\theta}_i^\intercal} (\boldsymbol{\theta}_i - \hat{\boldsymbol{\theta}}_i)} d\boldsymbol{\theta}_i \\ &amp; \approx e^{\ell(\hat{\boldsymbol{\theta}}_i)} (2 \pi)^{-T_i/2} N^{-T_i/2} \det \left(\frac{\partial^2 \bar{\ell}(\hat{\boldsymbol{\theta}}_i) }{\partial \boldsymbol{\theta}_i \partial \boldsymbol{\theta}_i^\intercal} \right)^{1/2}, \end{aligned}\] <p>where the second line uses a <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansion</a> around the MLE, and the third line executes the <a href="https://en.wikipedia.org/wiki/Gaussian_integral#n-dimensional_and_functional_generalization">multivariate Gaussian integral</a>. According to Schwarz this can be further simplified for large \(N\) as</p> \[p(\boldsymbol{y} | M_i) \propto e^{\ell(\hat{\boldsymbol{\theta}}_i)} N^{-T_i/2} = e^{-\tfrac{1}{2}\text{BIC}(M_i)},\] <p>where \(\text{BIC}(M_i) \equiv -2 \ell(\hat{\boldsymbol{\theta}}_i)) + T_i \log(N)\) is the <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">Bayesian Information Criterion</a> - a common in-sample measure used in identifying the “goodness of fit” of a model balanced against its complexity. Plugging this into \eqref{bayes} and taking a flat prior on the model space \(\mathbb{M}\) we finally arrive at the following expression for data-driven model probabilities:</p> <p>\begin{equation}\label{pm} p(M_i | \boldsymbol{y}) = \frac{e^{-\tfrac{1}{2}\text{BIC}(M_i)}}{ \sum_{j=1}^K e^{-\tfrac{1}{2}\text{BIC}(M_j)}}. \end{equation}</p> <p>Simply put: when all models have equal complexity we simply weigh their predictions based on their likelihood function.</p> <h3 id="towards-generality">Towards Generality</h3> <p>If practitioners of data science are put off by Bayesian information criterions, likelihood functions etc. I wouldn’t hold it against them. While these concepts are prevalent within statistics, they are less ubiquitous amongst computer scientists: indeed, they may not even be well-defined for many of the machine learning models commonly employed today. Further work is therefore warranted. Here, I’ll present a <a href="https://en.wikipedia.org/wiki/Wittgenstein%27s_ladder">Wittgenstein’s ladder</a>-type argument for what I think ought to be done.</p> <blockquote> <p>“<em>My propositions serve as elucidations in the following way: anyone who understands me eventually recognizes them as nonsensical, when he has used them—as steps—to climb beyond them. He must, so to speak, throw away the ladder after he has climbed up it</em>”.</p> </blockquote> <p>Suppose we have some model \(M_i: \boldsymbol{y}_t = f(\boldsymbol{x}_t \vert \boldsymbol{\theta}_i) + \varepsilon_t\) for \(t=1,2,...,N\) where \(\varepsilon_t \sim \mathcal{N}(0,\sigma_i^2)\) is an i.i.d. error term. The likelihood function for this model can be written as</p> \[L(\boldsymbol{\theta}_i) = \prod_{t=1}^N \frac{ e^{-\frac{(y_t - f(\boldsymbol{x}_t \vert \boldsymbol{\theta}_i))^2}{2\sigma_i^2}}}{\sqrt{2 \pi \sigma_i^2}},\] <p>or in log-likelihood terms:</p> \[\ell(\boldsymbol{\theta}_i) = - \frac{N}{2} \ln(2 \pi) - \frac{N}{2} \ln(\sigma_i^2) - \frac{1}{2\sigma_i^2} RSS_i,\] <p>where \(RSS_i\) is the <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares">residual sum of sqaures</a>. Now \(\sigma_i^2 \approx RSS_i/N = MSE_i\) (the mean squared error) so this expression boils down to</p> \[\ell(\boldsymbol{\theta}_i) = -\frac{N}{2} \ln(MSE_i) + \text{terms depending on }N.\] <p>Hence the Bayesian information criterion can be written as</p> \[BIC(M_i) = \frac{N}{2} \ln(MSE_i) + T_i \log(N).\] <p>This offers a somewhat more appealing way of writing \eqref{pm}, with the caveat that we still have to deal with quantifying model complexity (the \(T_i\) penalty term). Now in practice what I would do instead is the following: It is well known that minimising the BIC asymptotically is equivalent to leave-of-\(\nu\) cross-validation for linear models (see <a href="https://robjhyndman.com/hyndsight/crossvalidation/">this reference</a>). This suggests the following approach: for each model tune hyper-parameters using cross-validation. Rather than weighing predictions of the tuned models by their \(BIC\)-score, let’s weigh them by their cross-validated mean square error. Something as simple as</p> <p>\begin{equation}\label{pm2} p(M_i | \boldsymbol{y}) = \frac{ MSE_{i,cv}^{-1} }{ \sum_{j=1}^K MSE_{j,cv}^{-1}}, \end{equation}</p> <p>could do. The benefits of this are as follows: (a) it is extremely simple to calculate, and (b) no unfair advantage is given to over-fitting models.</p> <p>Again note that this argument is purely heuristic in nature. I welcome alternative suggestions to this intensely fascinating subject.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>For a more careful derivation I recommend Bhat and Kumar’s <a href="https://faculty.ucmerced.edu/hbhat/BICderivation.pdf">On the derivation of the Bayesian Information Criterion</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="modelling"/><category term="machine-learning"/><category term="probability"/><summary type="html"><![CDATA[Incorporating model uncertainty into your machine learning pipeline]]></summary></entry><entry><title type="html">Sub-optimal Optimal Portfolios</title><link href="https://sellersgaard.github.io/blog/2023/portfolio/" rel="alternate" type="text/html" title="Sub-optimal Optimal Portfolios"/><published>2023-03-27T12:00:00+00:00</published><updated>2023-03-27T12:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2023/portfolio</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2023/portfolio/"><![CDATA[<h3 id="a-perfect-brownian">A perfect Brownian</h3> <p>Imagine you are truthfully told about the functional form of the stochastic equations of motion characterising some set of stocks, with the caveat that you are required to estimate the associated parameters yourself. <em>Prima facie</em>, this complete removal of <a href="https://en.wikipedia.org/wiki/Knightian_uncertainty">Knightian uncertainty</a> sounds like a dream scenario which trivially will allow for what we vaguely can call “perfect trading”. In this piece I want to convince you that things are not quite so straight-forward. In particular, even with a substantial amount of data at your disposal, parameter uncertainty can still undermine theoretically optimal strategies. To cement this point, I will consider the simulated performance of a number of results from <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">modern portfolio theory</a>, both in the event that we <em>do</em> and <em>do not</em> have complete information about the input parameters. In a certain sense, this analysis is befitting considering that the father of modern portfolio theory, <a href="https://en.wikipedia.org/wiki/Harry_Markowitz">Harry Markowitz</a>, famously <a href="https://onlinelibrary.wiley.com/doi/10.1002/scin.5591791221">doesn’t practice what he preaches</a>, being instead partial towards “equal weight” diversification.</p> <p>To set the scene, let us suppose that the daily percentage returns of \(K\) assets are multivariate normal, \(\boldsymbol{R}_t \sim \mathcal{N}(\boldsymbol{\mu} \Delta t, \boldsymbol{\Sigma} \Delta t)\), where \(\boldsymbol{\mu} \in \mathbb{R}^K\) is an annualised mean return vector, and \(\boldsymbol{\Sigma} \in \mathbb{R}^{K \times K}\) is an annualised covariance matrix i.e. \(\boldsymbol{\Sigma} \equiv \text{diag}(\boldsymbol{\sigma})\boldsymbol{\varrho}\text{diag}(\boldsymbol{\sigma})\) where \(\boldsymbol{\sigma} \in \mathbb{R}^K\) is a volatility vector, and \(\boldsymbol{\varrho} \in \mathbb{R}^{K \times K}\) is a correlation matrix. \(\Delta t\) scales these quantities into daily terms and is typically set to 1/252 to reflect the average number of trading days in a year.</p> <p>Equivalently, we may write this in price process terms as the dynamics</p> \[\boldsymbol{S}_{t+1} = \boldsymbol{S}_{t} + \text{diag}(\boldsymbol{S}_{t})(\boldsymbol{\mu} \Delta t + \boldsymbol{L} \sqrt{\Delta t} \boldsymbol{Z}),\] <p>where \(\boldsymbol{L}\) is the lower triangular matrix arising from <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposing</a> \(\boldsymbol{\Sigma}\), and \(\boldsymbol{Z}: \Omega \mapsto \mathbb{R}^K\) is an i.i.d. standard normal vector i.e \(\boldsymbol{Z} \sim \mathcal{N}(\boldsymbol{0}, \mathbb{I}_{K})\).</p> <p>For the purpose of this exercise we shall consider five co-moving assets with the following parameter specifications:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">μ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.14</span><span class="p">])</span> <span class="c1">#mean
</span><span class="n">σ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span> <span class="c1">#volatility
</span><span class="n">ρ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">ρ</span> <span class="o">=</span> <span class="n">ρ</span><span class="o">+</span><span class="n">ρ</span><span class="p">.</span><span class="n">T</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">identity</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">μ</span><span class="p">))</span> <span class="c1">#correlation
</span><span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">252</span> <span class="c1">#1/trading days per year
</span><span class="n">T</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1">#simulation time [years] 
</span><span class="n">r</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">#risk-free rate 
</span><span class="n">S_0</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">μ</span><span class="p">))</span> <span class="c1">#initial share prices
</span><span class="n">W_0</span> <span class="o">=</span> <span class="mf">1e6</span> <span class="c1">#initial welath
</span></code></pre></div></div> <p>Furthermore, it will be convenient to define the following quantities:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">M</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">μ</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="n">dt</span><span class="p">)</span>
<span class="n">Σ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">σ</span><span class="p">)</span><span class="nd">@ρ@np.diag</span><span class="p">(</span><span class="n">σ</span><span class="p">)</span>
<span class="n">Σinv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Σ</span><span class="p">)</span>
<span class="n">sqdt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
<span class="n">λ</span><span class="p">,</span> <span class="n">Λ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eig</span><span class="p">(</span><span class="n">Σ</span><span class="p">)</span>
<span class="nf">assert </span><span class="p">(</span><span class="n">λ</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nf">all</span><span class="p">()</span> <span class="c1">#assert positive semi-definite
</span><span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">Σ</span><span class="p">)</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="n">Sharpe</span> <span class="o">=</span> <span class="p">(</span><span class="n">μ</span><span class="o">-</span><span class="n">r</span><span class="p">)</span><span class="o">/</span><span class="n">σ</span>
</code></pre></div></div> <p>Finally, to simulate stock paths according to these inputs we can use the following vectorised function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multi_brownian_sim</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">]:</span>

    <span class="k">if</span> <span class="n">seed</span><span class="p">:</span>
        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sqdt</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">L</span><span class="nd">@dW</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">μ</span><span class="o">*</span><span class="n">dt</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)).</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="p">.</span><span class="n">T</span>
    <span class="n">ret</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">S_0</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">ret</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">()</span> <span class="c1">#levels
</span>    <span class="n">dfr</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">diff</span><span class="p">()</span><span class="o">/</span><span class="n">df</span><span class="p">.</span><span class="nf">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">dropna</span><span class="p">()</span> <span class="c1">#returns 
</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">dfr</span>
</code></pre></div></div> <p>A single run of this yields something along the lines below:</p> <figure> <picture> <img src="/assets/img/brownian.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="estimating-parameters">Estimating parameters</h3> <p>Alright, suppose you are handed the data for these paths above and informed that returns are multivariate normal. What do you conclude? Well, you’ll probably find the unbiased estimators \(\{\hat{\boldsymbol{\mu}}, \hat{\boldsymbol{\sigma}}, \hat{\boldsymbol{\varrho}} \}\) as displayed in the 2nd column in the tables below. The noteworthy part here is how much some of these appear amiss vis-à-vis their population counterparts. So much so that one <em>almost suspects</em> that something is messed up with the data-generating process per se. To assure you this is not the case, we will do a couple of exercises: first, we will compute two-sigma confidence intervals for our estimators, and second, we will sample from the data-generating process repeatedly and show that the averages over the estimators indeed are congruent with the true underlying dynamics.</p> <p>Confidence intervals for \(\hat{\boldsymbol{\mu}}\) and \(\hat{\boldsymbol{\sigma}}^2\) can be computed using the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>: indeed, it is straightforward to show that \(\sqrt{N}(\hat{\mu}_i - \mu_i) \sim \mathcal{N}(0, \sigma_i^2)\) and \(\sqrt{N}(\hat{\sigma}_i^2 - \sigma_i^2) \sim \mathcal{N}(0, 2\sigma_i^4)\) where \(N\) is the number of observations. Deploying the <a href="https://en.wikipedia.org/wiki/Delta_method">delta method</a> to extract the limiting distribution of \(\hat{\boldsymbol{\sigma}}\) we find that \(\sqrt{N}(\hat{\sigma}_i - \sigma_i) \sim \mathcal{N}(0, \sigma_i^2/2)\). Finally, confidence intervals for \(\hat{\varrho}_{ij}\) can be estimated using the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Using_the_Fisher_transformation">Fisher transformation</a>.</p> <p>For the Monte Carlo simulation we run the data generating process 1000 times storing the estimators for \(\{\hat{\boldsymbol{\mu}}, \hat{\boldsymbol{\sigma}}, \hat{\boldsymbol{\varrho}} \}\) from each run in order to ultimately study their statistical properties. Per definition, 95% of the confidence intervals we’d compute from repeated sampling should encompass the true population parameters. Notice the expected values (6th column) across these samples align much more closely with the true parameters: a small victory for having the equivalent of 10,000 years’ worth of financial data(!)</p> <table border="1" class="dataframe" width="100%"> <thead> <tr style="text-align: right;"> <th></th> <th>$$\mu$$</th> <th>$$\hat{\mu}$$</th> <th>$$\hat{\mu}-2SE$$</th> <th>$$\hat{\mu}+2SE$$</th> <th>$$\in CI$$</th> <th>$$\mathbb{E}(\mu_{mc})$$</th> <th>$$\min(\mu_{mc})$$</th> <th>$$\max(\mu_{mc})$$</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0.10</td> <td>0.152</td> <td>0.090</td> <td>0.214</td> <td>True</td> <td>0.097</td> <td>0.003</td> <td>0.191</td> </tr> <tr> <th>1</th> <td>0.13</td> <td>0.096</td> <td>-0.000</td> <td>0.192</td> <td>True</td> <td>0.131</td> <td>-0.035</td> <td>0.252</td> </tr> <tr> <th>2</th> <td>0.08</td> <td>0.039</td> <td>-0.024</td> <td>0.103</td> <td>True</td> <td>0.080</td> <td>-0.027</td> <td>0.173</td> </tr> <tr> <th>3</th> <td>0.09</td> <td>0.105</td> <td>0.043</td> <td>0.168</td> <td>True</td> <td>0.089</td> <td>-0.019</td> <td>0.197</td> </tr> <tr> <th>4</th> <td>0.14</td> <td>0.132</td> <td>0.007</td> <td>0.257</td> <td>True</td> <td>0.139</td> <td>-0.039</td> <td>0.339</td> </tr> </tbody> </table> <p><br/></p> <table border="1" class="dataframe" width="100%"> <thead> <tr style="text-align: right;"> <th></th> <th>$$\sigma$$</th> <th>$$\hat{\sigma}$$</th> <th>$$\hat{\sigma}-2SE$$</th> <th>$$\hat{\sigma}+2SE$$</th> <th>$$\in CI$$</th> <th>$$\mathbb{E}(\sigma_{mc})$$</th> <th>$$\min(\sigma_{mc})$$</th> <th>$$\max(\sigma_{mc})$$</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0.10</td> <td>0.098</td> <td>0.096</td> <td>0.101</td> <td>True</td> <td>0.10</td> <td>0.096</td> <td>0.104</td> </tr> <tr> <th>1</th> <td>0.15</td> <td>0.152</td> <td>0.147</td> <td>0.156</td> <td>True</td> <td>0.15</td> <td>0.144</td> <td>0.158</td> </tr> <tr> <th>2</th> <td>0.10</td> <td>0.101</td> <td>0.098</td> <td>0.104</td> <td>True</td> <td>0.10</td> <td>0.096</td> <td>0.104</td> </tr> <tr> <th>3</th> <td>0.10</td> <td>0.099</td> <td>0.096</td> <td>0.101</td> <td>True</td> <td>0.10</td> <td>0.095</td> <td>0.104</td> </tr> <tr> <th>4</th> <td>0.20</td> <td>0.198</td> <td>0.192</td> <td>0.203</td> <td>True</td> <td>0.20</td> <td>0.191</td> <td>0.208</td> </tr> </tbody> </table> <p><br/></p> <table border="1" class="dataframe" width="100%"> <thead> <tr style="text-align: right;"> <th></th> <th></th> <th>$$\rho$$</th> <th>$$\hat{\rho}$$</th> <th>$$\hat{\rho}-2SE$$</th> <th>$$\hat{\rho}+2SE$$</th> <th>$$\in CI$$</th> <th>$$\mathbb{E}(\rho_{mc})$$</th> <th>$$\min(\rho_{mc})$$</th> <th>$$\max(\rho_{mc})$$</th> </tr> </thead> <tbody> <tr> <th rowspan="4" valign="top">0</th> <th>1</th> <td>0.20</td> <td>0.208</td> <td>0.169</td> <td>0.245</td> <td>True</td> <td>0.200</td> <td>0.146</td> <td>0.260</td> </tr> <tr> <th>2</th> <td>0.30</td> <td>0.292</td> <td>0.256</td> <td>0.328</td> <td>True</td> <td>0.299</td> <td>0.233</td> <td>0.365</td> </tr> <tr> <th>3</th> <td>0.10</td> <td>0.057</td> <td>0.017</td> <td>0.096</td> <td>False</td> <td>0.100</td> <td>0.039</td> <td>0.159</td> </tr> <tr> <th>4</th> <td>0.40</td> <td>0.405</td> <td>0.371</td> <td>0.438</td> <td>True</td> <td>0.400</td> <td>0.340</td> <td>0.456</td> </tr> <tr> <th rowspan="3" valign="top">1</th> <th>2</th> <td>0.50</td> <td>0.483</td> <td>0.452</td> <td>0.513</td> <td>True</td> <td>0.500</td> <td>0.447</td> <td>0.545</td> </tr> <tr> <th>3</th> <td>0.20</td> <td>0.189</td> <td>0.151</td> <td>0.227</td> <td>True</td> <td>0.200</td> <td>0.137</td> <td>0.256</td> </tr> <tr> <th>4</th> <td>0.10</td> <td>0.138</td> <td>0.099</td> <td>0.177</td> <td>True</td> <td>0.100</td> <td>0.037</td> <td>0.163</td> </tr> <tr> <th rowspan="2" valign="top">2</th> <th>3</th> <td>0.10</td> <td>0.124</td> <td>0.085</td> <td>0.163</td> <td>True</td> <td>0.101</td> <td>0.043</td> <td>0.164</td> </tr> <tr> <th>4</th> <td>0.15</td> <td>0.179</td> <td>0.140</td> <td>0.217</td> <td>True</td> <td>0.150</td> <td>0.093</td> <td>0.208</td> </tr> <tr> <th>3</th> <th>4</th> <td>0.20</td> <td>0.199</td> <td>0.160</td> <td>0.237</td> <td>True</td> <td>0.200</td> <td>0.139</td> <td>0.261</td> </tr> </tbody> </table> <p><br/> Scanning through the above numbers the most problematic estimator is clearly \(\hat{\boldsymbol{\mu}}\). Intuitively, we can explain this as follows: since \(R_t \equiv (S_t-S_{t-1})/S_{t-1} \approx \ln(S_t)-\ln(S_{t-1})\) it follows that</p> \[\bar{R} \equiv N^{-1} \sum_{i=1}^N R_t \approx N^{-1} \left( \ln(S_T)-\ln(S_{0}) \right),\] <p>i.e. the estimator is a <a href="https://en.wikipedia.org/wiki/Telescoping_series">telescoping series</a> effectively determined by the first and last entry. For “shorter” histories this can be extremely problematic (noisy).</p> <h3 id="the-final-frontier">The final frontier?</h3> <p>To get a sense of the sheer amount of gravity these uncertainties carry, it is helpful to consider the <a href="https://en.wikipedia.org/wiki/Efficient_frontier">efficient frontier</a> i.e. the collection of portfolio constructions which deliver the lowest amount of volatility for a given level of return. A well known result in modern portfolio theory states that this frontier carves out a hyperbola in (standard deviation, mean)-space given by the equation</p> \[\sigma_{\pi}^2(\mu_{\pi}) = \frac{C \mu_{\pi}^2 - 2B \mu_{\pi} + A}{D},\] <p>where \(A \equiv \boldsymbol{\mu}^\intercal \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}\), \(B \equiv \boldsymbol{\mu}^\intercal \boldsymbol{\Sigma}^{-1} \boldsymbol{1}\), \(C \equiv \boldsymbol{1}^\intercal \boldsymbol{\Sigma}^{-1} \boldsymbol{1}\), and \(D \equiv AC-B^2\).</p> <p>The function below allows us to visualise the efficient frontier for arbitrary \(\boldsymbol{\mu}\), \(\boldsymbol{\Sigma}\):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_frontier</span><span class="p">(</span><span class="n">μ_est</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">Σ_est</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">scatter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">muc</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>

    <span class="n">σ_est</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">Σ_est</span><span class="p">))</span>
    <span class="n">Σinv_est</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Σ_est</span><span class="p">)</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">μ_est</span><span class="nd">@Σinv_est@μ_est</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">μ_est</span><span class="nd">@Σinv_est@I</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">I</span><span class="nd">@Σinv_est@I</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="n">C</span> <span class="o">-</span> <span class="n">B</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">if</span> <span class="n">μ_est</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">muc</span><span class="p">:</span>
        <span class="n">muc</span> <span class="o">=</span> <span class="n">μ_est</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
    <span class="n">mubar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">muc</span><span class="p">,</span><span class="mf">0.005</span><span class="p">)</span>
    <span class="n">sigbar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="n">C</span><span class="o">*</span><span class="nf">pow</span><span class="p">(</span><span class="n">mubar</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="o">*</span><span class="n">mubar</span> <span class="o">+</span> <span class="n">A</span><span class="p">)</span><span class="o">/</span><span class="n">D</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ax</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="mi">11</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">sigbar</span><span class="p">,</span> <span class="n">mubar</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scatter</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">σ_est</span><span class="p">,</span> <span class="n">μ_est</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$\sigma$</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$\mu$</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Efficient Frontier</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>E.g. for the specified parameters characterising the five assets, this yields the following curve:</p> <figure> <picture> <img src="/assets/img/frontier1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now let’s consider what happens if \(\boldsymbol{\mu}\) is known <em>a priori</em>, but \(\boldsymbol{\Sigma}\) must be estimated from the data. As above, we’ll repeat the data generating process 1000 times, recomputing the frontier on each iteration, to get a sense of the variability.</p> <figure> <picture> <img src="/assets/img/frontier2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Evidently, while this introduces some estimation error, the overall picture remains “within reason”.</p> <p>Finally, suppose both \(\boldsymbol{\mu}\) are \(\boldsymbol{\Sigma}\) are inferred <em>a posteriori</em>. The Monte Carlo run yields:</p> <figure> <picture> <img src="/assets/img/frontier3.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>… which isn’t exactly great.</p> <p>The question remains: to what extent does this actually impact quantitative trading strategies? Well, the efficient frontier does include a couple of portfolios of interest, most notably the <em>minimum variance portfolio</em> and the <em>maximum Sharpe ratio portfolio</em>, both of which plausibly could be adopted by practitioners. Let’s see how such strategies fare in light of our current framework.</p> <h3 id="one-period-optimal-trading-strategies">One-period “optimal” trading strategies</h3> <p>One of the great tragedies of academic finance is the curious disconnect with which it operates from the practical field. Optimisation, for example, is a major division of mathematical finance, yet empirical tests thereof are shockingly scarce. So much so that the <em>enfant terrible</em> of the quant industry, Nassim Taleb, noted in “The Black Swan”:</p> <blockquote> <p><em>“I would not be the first to say that this optimisation set back the social science by reducing it from the intellectual and reflective discipline it was becoming to an attempt at an “exact science”. By “exact science”, I mean a second-rate engineering problem for those who want to pretend that they are in a physics department - so-called physics envy. In other words, an intellectual fraud”.</em></p> </blockquote> <p>To remedy this deplorable situation, let us finally explore how optimal one-period portfolio constructions fare against equal allocation (\(1/K\) diversification). In particular, let us consider whether (I) the minimum variance portfolio<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>:</p> \[\boldsymbol{\pi}_{\text{minvar}} = \frac{\boldsymbol{\Sigma}^{-1} \boldsymbol{1}}{ \boldsymbol{1}^\intercal \boldsymbol{\Sigma}^{-1} \boldsymbol{1}},\] <p>actually delivers a smaller variance than the \(1/K\) benchmark. For reference: \(\boldsymbol{\pi}_{\text{minvar}} \approx (0.32, 0.00, 0.31 , 0.38, -0.01)\). Similarly whether (II) the maximum sharpe ratio portfolio<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>:</p> \[\boldsymbol{\pi}_{\text{maxshp}} = \frac{\boldsymbol{\Sigma}^{-1} (\boldsymbol{\mu}-r\boldsymbol{1})}{ \boldsymbol{1}^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{\mu}-r\boldsymbol{1})},\] <p>actually delivers a higher Sharpe ratio than the \(1/K\) benchmark. For reference: \(\boldsymbol{\pi}_{\text{maxshp}} \approx (0.34, 0.14, 0.14, 0.33, 0.05)\).</p> <p>To analyse these questions, consider the following experimental set-up: Using simulated data we will run five <a href="https://en.wikipedia.org/wiki/Self-financing_portfolio">self-financing portfolios</a> for 10 years, each starting with a million dollar notational. The five strategies under scrutiny are: (i) The \(1/K\) portfolio, (ii) \(\boldsymbol{\pi}_{\text{minvar}}\) with known parameters, (iii) \(\boldsymbol{\pi}_{\text{minvar}}\) with unknown parameters, (iv) \(\boldsymbol{\pi}_{\text{maxshp}}\) with known parameters, and (v) \(\boldsymbol{\pi}_{\text{maxshp}}\) with unknown parameters. All estimated quantities will be based on at least ten years’ worth of history and will be updated weekly. At the end all portfolios will have their variance and Sharpe ratio computed. As usual, we’ll repeat this entire process 1000 times.</p> <p>For a given matrix of simulated returns, the following function computes the wealth paths of the five strategies:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">π_eql</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="o">/</span><span class="n">M</span> <span class="c1">#equally weighted portfolio
</span><span class="n">π_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">Σinv</span><span class="nd">@I</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">I</span><span class="nd">@Σinv@I</span><span class="p">)</span> <span class="c1">#minimum variance portfolio
</span><span class="n">π_shp</span> <span class="o">=</span> <span class="p">(</span><span class="n">Σinv</span><span class="o">@</span><span class="p">(</span><span class="n">μ</span><span class="o">-</span><span class="n">r</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">I</span><span class="nd">@Σinv</span><span class="o">@</span><span class="p">(</span><span class="n">μ</span><span class="o">-</span><span class="n">r</span><span class="p">))</span> <span class="c1">#optimal sharpe portfolio
</span>
<span class="k">def</span> <span class="nf">wealth_path</span><span class="p">(</span><span class="n">dfr</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">]:</span>

    <span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">W_equal</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">W_minvar</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">W_minvar_est</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">W_sharpe</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">W_sharpe_est</span><span class="sh">'</span><span class="p">,]</span>
    <span class="n">t_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dfw</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">t_test</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
    <span class="n">dfw</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">t_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],:]</span> <span class="o">=</span> <span class="n">W_0</span>
    <span class="n">pi_shp_dic</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pi_var_dic</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">μ_hat_vec</span> <span class="o">=</span> <span class="n">dfr</span><span class="p">.</span><span class="nf">expanding</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span><span class="o">/</span><span class="n">dt</span>
    <span class="n">Σ_hat_vec</span> <span class="o">=</span> <span class="n">dfr</span><span class="p">.</span><span class="nf">expanding</span><span class="p">().</span><span class="nf">cov</span><span class="p">()</span><span class="o">/</span><span class="n">dt</span>

    <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">t_test</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ti</span><span class="o">%</span><span class="mi">7</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">μ_hat</span> <span class="o">=</span> <span class="n">μ_hat_vec</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">values</span>
            <span class="n">Σ_hat</span> <span class="o">=</span> <span class="n">Σ_hat_vec</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">values</span>
            <span class="n">Σinv_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Σ_hat</span><span class="p">)</span>
            <span class="n">pi_shp_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Σinv_hat</span><span class="o">@</span><span class="p">(</span><span class="n">μ_hat</span><span class="o">-</span><span class="n">r</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">I</span><span class="nd">@Σinv_hat</span><span class="o">@</span><span class="p">(</span><span class="n">μ_hat</span><span class="o">-</span><span class="n">r</span><span class="p">))</span>
            <span class="n">pi_var_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Σinv_hat</span><span class="nd">@I</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">I</span><span class="nd">@Σinv_hat@I</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pi_shp_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_shp_dic</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">pi_var_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_var_dic</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">ti</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">col_map</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">'</span><span class="s">W_equal</span><span class="sh">'</span><span class="p">:</span> <span class="n">π_eql</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">W_minvar</span><span class="sh">'</span><span class="p">:</span> <span class="n">π_var</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">W_minvar_est</span><span class="sh">'</span><span class="p">:</span> <span class="n">pi_var_dic</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="sh">'</span><span class="s">W_sharpe</span><span class="sh">'</span><span class="p">:</span> <span class="n">π_shp</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">W_sharpe_est</span><span class="sh">'</span><span class="p">:</span> <span class="n">pi_shp_dic</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
                <span class="n">dfw</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">t</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dfw</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">col_map</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="nd">@dfr.loc</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>

    <span class="n">pi_shp</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="nf">from_dict</span><span class="p">(</span><span class="n">pi_shp_dic</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">index</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">pi_var</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="nf">from_dict</span><span class="p">(</span><span class="n">pi_var_dic</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">index</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dfw</span><span class="p">,</span> <span class="n">pi_shp</span><span class="p">,</span> <span class="n">pi_var</span>  
</code></pre></div></div> <p>Here’s one such example, which in itself won’t tell you much:</p> <figure> <picture> <img src="/assets/img/wealth.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>However, once we repeat this enough times a pattern starts to emerge. Specifically, consider the distributive properties of the optimal portfolios <em>less</em> the \(1/K\) benchmark:</p> <table border="1" class="dataframe" width="100%"> <thead> <tr style="text-align: right;"> <th></th> <th> $$\sigma(\boldsymbol{\pi}_{\text{minvar}})-\sigma(\boldsymbol{\pi}_{1/K} )$$ </th> <th> $$\sigma(\hat{\boldsymbol{\pi}}_{\text{minvar}})-\sigma(\boldsymbol{\pi}_{1/K} )$$ </th> <th> $$Shp(\boldsymbol{\pi}_{\text{maxshp}})-Shp(\boldsymbol{\pi}_{1/K} )$$ </th> <th> $$Shp(\hat{\boldsymbol{\pi}}_{\text{maxshp}})-Shp(\boldsymbol{\pi}_{1/K} )$$ </th> </tr> </thead> <tbody> <tr> <th>count</th> <td>1000</td> <td>1000</td> <td>1000</td> <td>1000</td> </tr> <tr> <th>mean</th> <td>-0.015</td> <td>-0.015</td> <td>0.104</td> <td>0.008</td> </tr> <tr> <th>std</th> <td>0.001</td> <td>0.001</td> <td>0.120</td> <td>0.161</td> </tr> <tr> <th>min</th> <td>-0.018</td> <td>-0.018</td> <td>-0.299</td> <td>-0.790</td> </tr> <tr> <th>2.5%</th> <td>-0.017</td> <td>-0.017</td> <td>-0.119</td> <td>-0.308</td> </tr> <tr> <th>50%</th> <td>-0.015</td> <td>-0.015</td> <td>0.105</td> <td>0.003</td> </tr> <tr> <th>97.5%</th> <td>-0.013</td> <td>-0.013</td> <td>0.342</td> <td>0.352</td> </tr> <tr> <th>max</th> <td>-0.013</td> <td>-0.013</td> <td>0.503</td> <td>0.641</td> </tr> <tr> <th># &lt; 0</th> <td>1000</td> <td>1000</td> <td>200</td> <td>493</td> </tr> </tbody> </table> <p><br/></p> <p>The take-away here is that the minimum variance portfolio does, in fact, deliver a lower volatility than the equal allocation portfolio - regardless of whether \(\boldsymbol{\Sigma}\) is estimated or not. As for the maximum Sharpe ratio portfolio the situation is more problematic: while 80% of the portfolios with known parameters <em>do</em> outperform the benchmark, there’s little to suggest optimisation actually helps once we introduce estimation into the picture. Again, the culprit is the uncertainty around the \(\hat{\boldsymbol{\mu}}\) estimator.</p> <p>In summary, even under highly idealised circumstances optimal trading strategies may not deliver as expected. Indeed, I would strongly caution against embracing small improvements in portfolio performance from some complicated optimisation process vs. keeping things simple. Markowitz’s \(1/K\) portfolio allocation is a lot less irrational than it initially appears.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I.e. the solution to \(\min_{\boldsymbol{\pi}} \boldsymbol{\pi}^\intercal \boldsymbol{\Sigma} \boldsymbol{\pi}\) where \(\boldsymbol{\pi}^\intercal \boldsymbol{1}=1\). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>I.e. the solution to \(\max_{\boldsymbol{\pi}} (\boldsymbol{\pi}^\intercal\mu - r)/\sqrt{\boldsymbol{\pi}^\intercal \boldsymbol{\Sigma} \boldsymbol{\pi}}\) where \(\boldsymbol{\pi}^\intercal \boldsymbol{1}=1\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="portfolio"/><category term="econometrics"/><category term="trading"/><summary type="html"><![CDATA[Exploring the effect of parameter uncertainty in optimal asset allocation]]></summary></entry><entry><title type="html">An Economist’s Guide to the Term Structure of Volatility</title><link href="https://sellersgaard.github.io/blog/2023/termstructure/" rel="alternate" type="text/html" title="An Economist’s Guide to the Term Structure of Volatility"/><published>2023-03-13T12:00:00+00:00</published><updated>2023-03-13T12:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2023/termstructure</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2023/termstructure/"><![CDATA[<h3 id="interpreting-the-vol-surface">Interpreting the vol surface</h3> <p>For anyone who dabbles in options, it is well-known that implied volatility manifestly is not flat, but rather a function of both the strike and the time to maturity: \(\sigma(K,T): \mathbb{R}^+ \times \mathbb{R}^+ \mapsto \mathbb{R}^+\). <em>A priori</em>, the existence of this surface might seem puzzling, but it is not in itself an inconsistency as long as certain <a href="https://mfe.baruch.cuny.edu/wp-content/uploads/2013/04/BloombergSVI2013.pdf">no-arbitrage bounds</a> are observed. Still, no-free-lunch arguments do little to explain <em>why</em> the surface is the way that it is. And I bet you won’t be blown away by hand-wavy arguments of supply meeting demand either.</p> <p>At this point we might interject that the surface is characterised by a number of stylized facts:</p> <ul> <li>For fixed maturities the surface tends to <em>smile</em> or <em>smirk</em> across the strike domain: i.e. out-of-the-money puts and calls tend to command a vol premium vis-à-vis their at-the-money counterpart. The intuition here is that the hedging demand for tail events in the underlying security drives up option prices in the wings. Viewed from a different angle, recall the Breeden-Litzenberger formula for the risk-neutral transition density: \(f^{\mathbb{Q}}(S_T=K | S_0)=\partial_{KK}^2 C(K,T)\). The volatility smile is effectively the market pricing in the probability of fat-tailed returns (famously, equity options had no smile prior to <a href="https://en.wikipedia.org/wiki/Black_Monday_(1987)">Black Monday</a> in ‘87).</li> <li>For fixed strikes the volatility surface tends to increase with maturity, i.e. the volatility term structure favours <a href="https://www.investopedia.com/articles/07/contango_backwardation.asp">contango over backwardation</a> (e.g. futures on the <a href="https://en.wikipedia.org/wiki/VIX">VIX Index</a> exist in a state of contango 70% of the time). The reason for the back-end of the curve being bid up, can again be explained in hedging terms: over longer horizons the market perceives an increased risk of unforeseeable asset price shocks, and long derivative positions are entered accordingly. Note that the slope of the curve simultaneously incentivises investors to enter short vol carry strategies at the front end - a strategy sometimes referred to as “picking up pennies in front of a steam roller”.</li> </ul> <h3 id="term-time">Term-time</h3> <p>In this piece I wish to dig a little deeper into the term structure: while I believe the above argument essentially is correct, it misses one key ingredient, viz. the effect of market moving <em>events</em> (be they recurrent (CPI, NFP, ISM,…) or one-off (Brexit, Trump(?), The ETH Merge,…)). To see how this is incorporated it will be helpful to view volatility not as an entity in its own right, but rather as a sum of daily <a href="https://en.wikipedia.org/wiki/Forward_volatility">forward volatility</a> components:</p> \[\sigma_{T} \equiv \sqrt{\frac{1}{T} \sum_{i=1}^T \mathbb{E}_0 [\sigma_{i-1,i}^2]}.\] <p>Intuitively this is appealing because it allows us to think about how much volatility the market expect to materialise on any given day. For example, in the absence of events, imagine that the market prices in a simple mean-reverting (“<a href="https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#GARCH">GARCH(1,1)</a>”) process:</p> <p>\begin{equation}\label{vmdl} \sigma_{i,i+1}^2 = \theta^2 + \beta(\sigma_{i-1,i}^2 - \theta^2) + \eta_i, \end{equation}</p> <p>where \(\theta\) is the long-run volatility, \(\beta \in (0,1)\) codifies the speed of mean-reversion, and \(\eta\) is a mean-zero innovation term. Through repeated substitution we find that</p> \[\begin{aligned} \mathbb{E}_0 [\sigma_{i-1,i}^2] &amp;= \mathbb{E}_0 [\theta^2 + \beta(\sigma_{i-2,i-1}^2 - \theta^2) + \eta_{i-1}] \\ &amp;= \theta^2 + \beta( \mathbb{E}_0[\sigma_{i-2,i-1}^2] - \theta^2) \\ &amp;= \theta^2 + \beta( \mathbb{E}_0[\theta + \beta(\sigma_{i-3,i-2}^2 - \theta^2) + \eta_{i-2}] - \theta^2) \\ &amp;=\theta^2 + \beta^2(\mathbb{E}_0[\sigma_{i-3,i-2}^2] - \theta^2) \\ &amp;= \hspace{3mm}... \\ &amp;= \theta^2 + \beta^{i-1}(\sigma_{0,1}^2 - \theta^2). \\ \end{aligned}\] <p>Remarkably, this rather primitive model of the forward term structure is often sufficient for observable reality, bar a number of spikes associated with event risks. To accommodate these in our model, we write</p> \[\mathbb{E}_0 [\sigma_{i-1,i}^2] = \theta^2 + \beta^{i-1}(\sigma_{0,1}^2 - \theta^2) + \delta_{i-1,i},\] <p>where \(\delta_{i-1,i} \geq 0\) is an idiosyncratic adjustment for the day. In other words, \(\delta\) captures the <em>excess volatility</em> we expect to induce through the introduction of the event vs. an otherwise “normal” day.</p> <p>Finally, substituting into for our expression for \(\sigma_T\) we find that the volatility term structure is</p> \[\sigma_{T} = \sqrt{ \theta^2 + \frac{1-\beta^T}{1-\beta} \cdot \frac{\sigma_{0,1}^2-\theta^2}{T} + \bar{\Delta}_T},\] <p>where \(\bar{\Delta}_T = T^{-1} \sum_{i=1}^T \delta_{i-1,i}\).</p> <p>In the example below we consider: (I) a baseline forward volatility term structure with \(\sigma_{0,1}=0.3\), \(\theta=0.4\), and \(\beta=0.92\). (II) The inclusion of two bimonthly events, with respective multiplier effects of 1.1 and 1.2 measured from the baseline. (III) The final volatility term structure, averaging over the forward volatility components.</p> <figure> <picture> <img src="/assets/img/volterm.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="extensions">Extensions</h3> <p>Sometimes a simple monotonic model for the baseline forward term structure is not expressively adequate. E.g. if the forward term structure is spoon shaped, we may take this as an indicator that market prices in reversion to <em>one mean</em> in the short run, and <em>another mean</em> in the long run. Fortunately, this effect is easy to incorporate. In <a href="https://www.sciencedirect.com/science/article/abs/pii/S0304405X0800144X">Christoffersen et al. (2008)</a> the following extension to \eqref{vmdl} is proposed:</p> <p>\begin{equation}\label{vmdl2} \sigma_{i,i+1}^2 = \theta_{i,i+1}^2 + \beta(\sigma_{i-1,i}^2 - \theta_{i-1,i}^2) + \eta_i^{(1)}, \hspace{5mm} <br/> \theta_{i,i+1}^2 = \omega^2 + \rho \theta_{i-1,i}^2 + \eta_i^{(2)}, \end{equation}</p> <p>in which \(\theta\) in itself has been elevated to a stochastic, mean-reverting process (assume \(\omega&gt;0\) and \(\rho \in (0,1)\)). Through repeated substitution we readily infer that</p> \[\mathbb{E}_0 [\sigma_{i-1,i}^2] = \omega^2 + \rho^{i-1}(\theta_{0,1}^2 - \omega^2) + \beta^{i-1}(\sigma_{0,1}^2 - \theta_{0,1}^2),\] <p>which we complete by appending a \(\delta\) correction term like before. The volatility term structure can therefore be expressed as</p> \[\sigma_{T} = \sqrt{ \omega^2 + \frac{1-\rho^T}{1-\rho} \cdot \frac{\theta_{0,1}^2-\omega^2}{T} + \frac{1-\beta^T}{1-\beta} \cdot \frac{\sigma_{0,1}^2-\theta_{0,1}^2}{T} + \bar{\Delta}_T}.\] <p>What I find particularly neat about this model, is its close kinship with the yield curve formula proposed by <a href="https://www.jstor.org/stable/2352957">Nelson and Siegel (1987)</a>. To see this, note that in continuous time \eqref{vmdl2} emounts to something like</p> \[d\sigma_t^2 = d\theta_t^2 + \beta(\theta_t^2 - \sigma_t^2)dt + \eta^{(1)} dW_t^{(1)}, \hspace{5mm} d\theta_t^2 = \rho(\omega^2 - \theta_t^2) dt + \eta^{(2)} dW_t^{(2)},\] <p>(yes, this is abuse of notation: the constants you see have been re-defined to keep things simple). Solving for the expected future variance we find</p> \[\mathbb{E}_0[\sigma_T^2] = \omega^2 + e^{-\rho T}(\theta_0^2 - \omega^2) + e^{-\beta T}(\sigma_0^2 - \theta_0^2)\] <p>or in simple terms:</p> \[\mathbb{E}_0[\sigma_T^2] = c_1 + c_2 e^{-\rho T} + c_3 e^{-\beta T},\] <p>where the \(c_i\)s are constants. Rewriting \(e^{-\beta T}\) as \(e^{-\rho T}e^{(\rho-\beta) T} \approx e^{-\rho T} (1+(\rho-\beta) T)\) we have to a first order of approximation</p> \[\mathbb{E}_0[\sigma_T^2] = c_1 + c_2' e^{-\rho T} + c_3' \rho T e^{-\rho T},\] <p>which I submit is equivalent to the Nelson-Siegel definition of the instantaneous forward rate. In particular, solving for \(T^{-1} \int_0^T \mathbb{E}_0[\sigma_t^2] dt\) we obtain an expression for the implied variance at time \(T\):</p> \[\sigma_{\text{impl},T}^2 = c_1 + c_2' \left( \frac{1-e^{-\rho T}}{\rho T} \right) + c_3' \left( \frac{1-e^{-\rho T}}{\rho T} - e^{-\rho T} \right).\] <h3 id="conclusion">Conclusion</h3> <p>Viewing the volatility term structure through the lens of forward volatility components with event corrections offers a considerably more economically intuitive approach to volatility. Understanding how these event corrections are determined is an interesting research question, which I will pass over in silence.</p>]]></content><author><name></name></author><category term="pricing"/><category term="options"/><category term="volatility"/><category term="econometrics"/><summary type="html"><![CDATA[How macro events are baked into the volatility surface]]></summary></entry><entry><title type="html">Black-Scholes, Measured Carefully</title><link href="https://sellersgaard.github.io/blog/2023/probmeasure/" rel="alternate" type="text/html" title="Black-Scholes, Measured Carefully"/><published>2023-03-01T12:00:00+00:00</published><updated>2023-03-01T12:00:00+00:00</updated><id>https://sellersgaard.github.io/blog/2023/probmeasure</id><content type="html" xml:base="https://sellersgaard.github.io/blog/2023/probmeasure/"><![CDATA[<h3 id="the-long-and-winding-road">The long and winding road</h3> <p>For anyone who has ever gone through the trouble of deriving the <a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model#Black%E2%80%93Scholes_formula">Black-Scholes formula</a> from first principles, this process will be recognised as tedious and requiring some attention to detail. This largely holds true, even if we assume the correctness of the <a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model#Black%E2%80%93Scholes_equation">Black-Scholes partial differential equation</a> and the associated Feynman-Kac solution. Specifically, let us assume that the time zero value of a call option can be expressed as the expectation of the discounted payoff,</p> <p>\begin{equation}\label{call} C_0 = \mathbb{E}^{\mathbb{Q}} [ e^{-rT} (S_T-K)^+ ] , \end{equation}</p> <p>where \(\mathbb{Q}\) is defined through the Radon-Nikodym derivative</p> \[\left.{\frac{d \mathbb{Q}}{d \mathbb{P}}}\right|_t \equiv e^{ -\tfrac{1}{2} \lambda^2 t - \lambda W_t},\] <p>and \(\lambda \equiv \frac{\mu-r}{\sigma}\) is the market price of risk. We are still left with (i) determining the exact (non-relational) form risk-neutral density and (ii) evaluating the resulting integrals. As for part (i), I personally prefer taking the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">unconscious approach</a> by first expressing the terminal value of the underlying as a function of a standard normal random variable, \(Z \sim N(0,1)\), viz.</p> \[S_T(Z) = S_0 e^{ (r-\tfrac{1}{2}\sigma^2)T + \sigma \sqrt{T} Z },\] <p>which follows by applying <a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma#It%C3%B4_drift-diffusion_processes_(due_to:_Kunita%E2%80%93Watanabe)">Itô’s Lemma</a> to \(\ln S_t\). The appropriate density to use is therefore the standard normal distribution, \(\phi(z)=\frac{e^{-z^2/2}}{\sqrt{2\pi}}\).<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> As for part (ii), we need to evaluate the integral</p> \[C_0 = e^{-rT} \int_{\mathbb{R}} \left( S_T(z) -K \right)\boldsymbol{1}\{S_T(z) \geq K\} \phi(z) dz,\] <p>where \(\boldsymbol{1}\{ \cdot\}\) is the indicator function. The \(K\) term is trivially expressed through the cumulative normal distribution. On the other hand, the \(S_T(z)\) term can only be expressed as such after we first complete the square in the exponent and then perform a change of variables. I won’t reproduce the details here, but hopefully the reader catches my drift.</p> <h3 id="whats-your-measure-sir">What’s your measure, sir?</h3> <p>What I am really trying to get at is the observation that a firm foundation in probability theory will lead us to the Black-Scholes formula faster and more elegantly. In fact, we can obtain the desired result without doing any integration whatsoever. The trick here is to think carefully about which probability measure we are using: in particular, moving beyond the mindset that all valuation must be done exclusively under the risk neutral measure.</p> <p>Recall that in the general no-arbitrage relation for contingent claims,</p> <p>\begin{equation}\label{hi} V_0 = \frac{V_0}{e^{r \cdot 0}} = \mathbb{E}^{\mathbb{Q}} \left[ \frac{V_T}{e^{rT}} \right], \end{equation}</p> <p>asset prices measured in units of the risk free asset (\(B_t=e^{rt}\)) are \(\mathbb{Q}\)-martingales. Remarkably, it transpires that <a href="https://en.wikipedia.org/wiki/Equivalence_(measure_theory)">equivalent probability measures</a> exist such that asset prices, measured in units of other tradeable assets, likewise are martingales. To see this, observe that the quantity</p> \[e^{rt} \frac{S_t}{S_0} = e^{-\tfrac{1}{2} \sigma^2 t + \sigma W_t^{\mathbb{Q}}},\] <p>is a positive \(\mathbb{Q}\)-martingale with unit expectation. Thus, <a href="https://en.wikipedia.org/wiki/Girsanov_theorem">Girsanov’s theorem</a> tells us that we can define an equivalent measure \(\mathbb{Q}^S \sim \mathbb{Q}\):</p> \[\left.{\frac{d \mathbb{Q}^S}{d \mathbb{Q}}}\right|_t \equiv e^{-rt} \frac{S_t}{S_0},\] <p>such that</p> <p>\begin{equation}\label{gs} W_t^{\mathbb{Q}^S} = W_t^{\mathbb{Q}} - \sigma t. \end{equation}</p> <p>We can therefore express \eqref{hi} as</p> \[V_0 = \int \frac{V_T}{e^{rT}} d \mathbb{Q}_T = \int \frac{V_T}{e^{rT}} e^{rT} \frac{S_0}{S_T} d\mathbb{Q}_T^S = S_0 \int \frac{V_T}{S_T} d\mathbb{Q}_T^S,\] <p>or identically</p> \[\frac{V_0}{S_0} = \mathbb{E}^{\mathbb{Q}^S} \left[ \frac{V_T}{S_T} \right],\] <p>which is what we wanted to show. As the unit of account has been changed, we refer to this process as a change of <a href="https://www.investopedia.com/terms/n/numeraire.asp">numeraire</a>.</p> <p>To see how this aids in the derivation of the Black-Scholes formula, let us write \eqref{call} as</p> \[C_0 = \mathbb{E}^{\mathbb{Q}}[e^{-rT} S_T \boldsymbol{1}\{S_T \geq K\}] - e^{-rT}K \mathbb{E}^{\mathbb{Q}}[ \boldsymbol{1}\{S_T \geq K\}].\] <p>The latter expectation is trivially identified as \(\mathbb{Q}(S_T \geq K)\) i.e. the risk neutral probability of the option expiring in the money. Meanwhile, for the first term, it is clearly advantageous to value the stock - not in units of the risk free asset - but rather in units of the stock itself. In other words: to perform a change of measure from \(\mathbb{Q}\) to \(\mathbb{Q}^S\). Doing so the first term becomes \(S_0 \mathbb{Q}^S(S_T \geq K)\), which is the present value of the stock, weighted by the stock-measure probability of the option expiring in the money. Jointly, we therefore have the fairly appealing pricing relation:</p> \[C_0 = S_0 \mathbb{Q}^S(S_T \geq K) - e^{-rT}K\mathbb{Q}(S_T \geq K).\] <p>Now observe that under the risk neutral measure \(\ln(S_T)\) is normally distributed with mean \(m=\ln(S_0) + (r-\tfrac{1}{2}\sigma^2)T\) and variance \(v^2 = \sigma^2T\). Letting \(Z\) be a standard normal random variable we find that</p> \[\mathbb{Q}(S_T \geq K) = \mathbb{Q}(m + vZ \geq \ln(K)) = 1-\mathbb{Q}(Z \leq \tfrac{\ln(K)-m}{v}) = \Phi(d_2),\] <p>where \(\Phi(\cdot)\) is the cumulative normal distribution, and \(d_2\) has the usual definition:</p> \[d_2 \equiv \frac{\ln(S_0/K) + (r-\tfrac{1}{2}\sigma^2)T}{\sigma \sqrt{T}}.\] <p>In a similar vein, under the stock measure \(\ln(S_T)\) is normally distributed, this time with mean \(m=\ln(S_0) + (r+\tfrac{1}{2}\sigma^2)T\) and variance \(v^2 = \sigma^2T\) (cf. eqn. \eqref{gs}). This immediately allows us to deduce</p> \[\mathbb{Q}^S(S_T \geq K) = \Phi(d_1),\] <p>where</p> \[d_1 \equiv d_2 + \sigma \sqrt{T}.\] <p>Taken together, the Black-Scholes formula is manifest:</p> \[C_0 = S_0 \Phi(d_1) - e^{-rT}K \Phi(d_2).\] <h3 id="parting-thoughts">Parting thoughts</h3> <p>Emphatically, the power of measure/numeraire changes transcends the simple example provided here. Valuing an option on a coupon bearing bond? Consider using a zero-coupon bond as a numeraire. Valuing a swaption? Using an annuity as a numeraire will make your life easier. Plenty of such examples exist, and I encourage you to explore such possibilities when tasked with a valuation exercise.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Alternatively we could keep \(S_T\) and work directly with the transition density \(d{\mathbb{Q}}(S_T \vert S_0)\). To this end, we could solve the Fokker-Planck equation as demonstrated <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion#Properties">here</a>. I’ll leave it as an exercise to the reader to show that the two approaches are in fact equivalent. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="pricing"/><category term="stochastic-calculus"/><category term="probability"/><summary type="html"><![CDATA[Going beyond the risk neutral probability distribution]]></summary></entry></feed>